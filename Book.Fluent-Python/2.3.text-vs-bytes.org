** Chapter 4. Text versus Bytes


The GNU/Linux kernel is not Unicode savvy, so in the real world you may find filenames made of byte sequences that are not valid in any sensible encoding scheme, and cannot be decoded to =str=. File servers with clients using a variety of OSes are particularly prone to this problem.

In order to work around this issue, all =os= module functions that accept filenames or pathnames take arguments as =str= or =bytes=. If one such function is called with a =str= argument, the argument will be automatically converted using the codec named by =sys.getfilesystemencoding()=, and the OS response will be decoded with the same codec. This is almost always what you want, in keeping with the Unicode sandwich best practice.

But if you must deal with (and perhaps fix) filenames that cannot be handled in that way, you can pass =bytes= arguments to the =os= functions to get =bytes= return values. This feature lets you deal with any file or pathname, no matter how many gremlins you may find. See [[file:ch04.html#ex_listdir1][Example 4-23]].



Example 4-23. listdir with str and bytes arguments and results

#+BEGIN_EXAMPLE
    >>> os.listdir('.')  # 
    ['abc.txt', 'digits-of-π.txt']
    >>> os.listdir(b'.')  # 
    [b'abc.txt', b'digits-of-xcfx80.txt']
#+END_EXAMPLE

- [[#CO48-1][[[file:callouts/1.png]]]]  :: The second filename is “digits-of-π.txt” (with the Greek letter pi).

- [[#CO48-2][[[file:callouts/2.png]]]]  :: Given a =byte= argument, =listdir= returns filenames as bytes: =b'xcfx80'= is the UTF-8 encoding of the Greek letter pi).

To help with manual handling of =str= or =bytes= sequences that are file or pathnames, the =os= module provides special encoding and decoding functions:

-  =fsencode(filename)=  :: Encodes =filename= (can be =str= or =bytes=) to =bytes= using the codec named by =sys.getfilesystemencoding()= if =filename= is of type =str=, otherwise returns the =filename= =bytes= unchanged.
-  =fsdecode(filename)=  :: Decodes =filename= (can be =str= or =bytes=) to =str= using the codec named by =sys.getfilesystemencoding()= if =filename= is of type =bytes=, otherwise returns the =filename= =str= unchanged.

On Unix-derived platforms, these functions use the =surrogateescape= error handler (see the sidebar that follows) to avoid choking on unexpected bytes. On Windows, the =strict= error handler is used.

Using surrogateescape to Deal with Gremlins

A trick to deal with unexpected bytes or unknown encodings is the =surrogateescape= codec error handler described in [[https://www.python.org/dev/peps/pep-0383/][PEP 383 --- Non-decodable Bytes in System Character Interfaces]] introduced in Python 3.1.

The idea of this error handler is to replace each nondecodable byte with a code point in the Unicode range from U+DC00 to U+DCFF that lies in the so-called “Low Surrogate Area” of the standard---a code space with no characters assigned, reserved for internal use in applications. On encoding, such code points are converted back to the byte values they replaced. See [[file:ch04.html#ex_listdir][Example 4-24]].



Example 4-24. Using surrogatescape error handling

#+BEGIN_EXAMPLE
    >>> os.listdir('.')  
    ['abc.txt', 'digits-of-π.txt']
    >>> os.listdir(b'.')  
    [b'abc.txt', b'digits-of-xcfx80.txt']
    >>> pi_name_bytes = os.listdir(b'.')[1]  
    >>> pi_name_str = pi_name_bytes.decode('ascii', 'surrogateescape')  
    >>> pi_name_str  
    'digits-of-udccfudc80.txt'
    >>> pi_name_str.encode('ascii', 'surrogateescape')  
    b'digits-of-xcfx80.txt'
#+END_EXAMPLE

- [[#CO49-1][[[file:callouts/1.png]]]]  :: List directory with a non-ASCII filename.

- [[#CO49-2][[[file:callouts/2.png]]]]  :: Let's pretend we don't know the encoding and get filenames as =bytes=.

- [[#CO49-3][[[file:callouts/3.png]]]]  :: =pi_names_bytes= is the filename with the pi character.

- [[#CO49-4][[[file:callouts/4.png]]]]  :: Decode it to =str= using the ='ascii'= codec with ='surrogateescape'=.

- [[#CO49-5][[[file:callouts/5.png]]]]  :: Each non-ASCII byte is replaced by a surrogate code point: ='xcfx80'= becomes ='udccfudc80'=.

- [[#CO49-6][[[file:callouts/6.png]]]]  :: Encode back to ASCII bytes: each surrogate code point is replaced by the byte it replaced.

This ends our exploration of =str= and =bytes=. If you are still with me, congratulations!

** Chapter Summary


We started the chapter by dismissing the notion that =1 character == 1 byte=. As the world adopts Unicode (80% of websites already use UTF-8), we need to keep the concept of text strings separated from the binary sequences that represent them in files, and Python 3 enforces this separation.

After a brief overview of the binary sequence data types---=bytes=, =bytearray=, and =memoryview=---we jumped into encoding and decoding, with a sampling of important codecs, followed by approaches to prevent or deal with the infamous =UnicodeEncodeError=, =UnicodeDecodeError=, and the =SyntaxError= caused by wrong encoding in Python source files.

While on the subject of source code, I presented my position on the debate about non-ASCII identifiers: if the maintainers of the code base want to use a human language that has non-ASCII characters, the identifiers should follow suit---unless the code needs to run on Python 2 as well. But if the project aims to attract an international contributor base, identifiers should be made from English words, and then ASCII suffices.

We then considered the theory and practice of encoding detection in the absence of metadata: in theory, it can't be done, but in practice the Chardet package pulls it off pretty well for a number of popular encodings. Byte order marks were then presented as the only encoding hint commonly found in UTF-16 and UTF-32 files---sometimes in UTF-8 files as well.

In the next section, we demonstrated opening text files, an easy task except for one pitfall: the =encoding== keyword argument is not mandatory when you open a text file, but it should be. If you fail to specify the encoding, you end up with a program that manages to generate “plain text” that is incompatible across platforms, due to conflicting default encodings. We then exposed the different encoding settings that Python uses as defaults and how to detect them: =locale.getpreferredencoding()=, =sys.getfilesystemencoding()=, =sys.getdefaultencoding()=, and the encodings for the standard I/O files (e.g., =sys.stdout.encoding=). A sad realization for Windows users is that these settings often have distinct values within the same machine, and the values are mutually incompatible; GNU/Linux and OSX users, in contrast, live in a happier place where =UTF-8= is the default pretty much everywhere.

Text comparisons are surprisingly complicated because Unicode provides multiple ways of representing some characters, so normalizing is a prerequisite to text matching. In addition to explaining normalization and case folding, we presented some utility functions that you may adapt to your needs, including drastic transformations like removing all accents. We then saw how to sort Unicode text correctly by leveraging the standard =locale= module---with some caveats---and an alternative that does not depend on tricky locale configurations: the external PyUCA package.

Finally, we glanced at the Unicode database (a source of metadata about every character), and wrapped up with brief discussion of dual-mode APIs (e.g., the =re= and =os= modules, where some functions can be called with =str= or =bytes= arguments, prompting different yet fitting results).

** Further Reading


Ned Batchelder's 2012 PyCon US talk [[http://nedbatchelder.com/text/unipain.html][“Pragmatic Unicode --- or --- How Do I Stop the Pain?”]] was outstanding. Ned is so professional that he provides a full transcript of the talk along with the slides and video. Esther Nam and Travis Fischer gave an excellent PyCon 2014 talk “Character encoding and Unicode in Python: How to (╯°□°)╯︵ ┻━┻ with dignity” ([[http://bit.ly/1JzF1MY][slides]], [[http://bit.ly/1JzF37P][video]]), from which I quoted this chapter's short and sweet epigraph: “Humans use text. Computers speak bytes.” Lennart Regebro---one of this book's technical reviewers---presents his “Useful Mental Model of Unicode (UMMU)” in the short post [[https://regebro.wordpress.com/2011/03/23/unconfusing-unicode-what-is-unicode/][“Unconfusing Unicode: What Is Unicode?”]]. Unicode is a complex standard, so Lennart's UMMU is a really useful starting point.

The official [[https://docs.python.org/3/howto/unicode.html][Unicode HOWTO]] in the Python docs approaches the subject from several different angles, from a good historic intro to syntax details, codecs, regular expressions, filenames, and best practices for Unicode-aware I/O (i.e., the Unicode sandwich), with plenty of additional reference links from each section. [[http://www.diveintopython3.net/strings.html][Chapter 4, “Strings”]], of Mark Pilgrim's awesome book [[http://www.diveintopython3.net][/Dive into Python 3/]] also provides a very good intro to Unicode support in Python 3. In the same book, [[http://bit.ly/1IqJ63d][Chapter 15]] describes how the Chardet library was ported from Python 2 to Python 3, a valuable case study given that the switch from the old =str= to the new =bytes= is the cause of most migration pains, and that is a central concern in a library designed to detect encodings.

If you know Python 2 but are new to Python 3, Guido van Rossum's [[http://bit.ly/1IqJ8YH][What's New in Python 3.0]] has 15 bullet points that summarize what changed, with lots of links. Guido starts with the blunt statement: “Everything you thought you knew about binary data and Unicode has changed.” Armin Ronacher's blog post [[http://bit.ly/1IqJcrD][“The Updated Guide to Unicode on Python”]] is deep and highlights some of the pitfalls of Unicode in Python 3 (Armin is not a big fan of Python 3).

Chapter 2, “Strings and Text,” of the /[[http://shop.oreilly.com/product/0636920027072.do][Python Cookbook, Third Edition]]/ (O'Reilly), by David Beazley and Brian K. Jones, has several recipes dealing with Unicode normalization, sanitizing text, and performing text-oriented operations on byte sequences. Chapter 5 covers files and I/O, and it includes “Recipe 5.17. Writing Bytes to a Text File,” showing that underlying any text file there is always a binary stream that may be accessed directly when needed. Later in the cookbook, the =struct= module is put to use in “Recipe 6.11. Reading and Writing Binary Arrays of Structures.”

Nick Coghlan's Python Notes blog has two posts very relevant to this chapter: [[http://bit.ly/1dYuNJa][“Python 3 and ASCII Compatible Binary Protocols”]] and [[http://bit.ly/1dYuRbS][“Processing Text Files in Python 3”]]. Highly recommended.

Binary sequences are about to gain new constructors and methods in Python 3.5, with one of the current constructor signatures being deprecated (see [[https://www.python.org/dev/peps/pep-0467/][PEP 467 --- Minor API improvements for binary sequences]]). Python 3.5 should also see the implementation of [[https://www.python.org/dev/peps/pep-0461/][PEP 461 --- Adding % formatting to bytes and bytearray]].

A list of encodings supported by Python is available at [[https://docs.python.org/3/library/codecs.html#standard-encodings][Standard Encodings]] in the =codecs= module documentation. If you need to get that list programmatically, see how it's done in the [[http://bit.ly/1IqKrqD][//Tools/unicode/listcodecs.py/]] script that comes with the CPython source code.

Martijn Faassen's [[http://bit.ly/1IqKu5I][“Changing the Python Default Encoding Considered Harmful”]] and Tarek Ziadé's [[http://blog.ziade.org/2008/01/08/syssetdefaultencoding-is-evil/][“sys.setdefaultencoding Is Evil”]] explain why the default encoding you get from =sys.getdefaultencoding()= should never be changed, even if you discover how.

The books /[[http://shop.oreilly.com/product/9780596101213.do][Unicode Explained]]/ by Jukka K. Korpela (O'Reilly) and [[http://bit.ly/1dYveDl][/Unicode Demystified/]] by Richard Gillam (Addison-Wesley) are not Python-specific but were very helpful as I studied Unicode concepts. [[http://unicodebook.readthedocs.org/index.html][/Programming with Unicode/]] by Victor Stinner is a free, self-published book (Creative Commons BY-SA) covering Unicode in general as well as tools and APIs in the context of the main operating systems and a few programming languages, including Python.

The W3C pages [[http://www.w3.org/International/wiki/Case_folding][Case Folding: An Introduction]] and [[http://www.w3.org/TR/charmod-norm/][Character Model for the World Wide Web: String Matching and Searching]] cover normalization concepts, with the former being a gentle introduction and the latter a working draft written in dry standard-speak---the same tone of the [[http://unicode.org/reports/tr15/][Unicode Standard Annex #15 --- Unicode Normalization Forms]]. The [[http://www.unicode.org/faq/normalization.html][Frequently Asked Questions / Normalization]] from [[http://www.unicode.org/][Unicode.org]] is more readable, as is the [[http://www.macchiato.com/unicode/nfc-faq][NFC FAQ]] by Mark Davis---author of several Unicode algorithms and president of the Unicode Consortium at the time of this writing.

Soapbox

*What Is “Plain Text”?*

For anyone who deals with non-English text on a daily basis, “plain text” does not imply “ASCII.” The [[http://www.unicode.org/glossary/#plain_text][Unicode Glossary]] defines /plain text/ like this:

#+BEGIN_QUOTE
  Computer-encoded text that consists only of a sequence of code points from a given standard, with no other formatting or structural information.
#+END_QUOTE

That definition starts very well, but I don't agree with the part after the comma. HTML is a great example of a plain-text format that carries formatting and structural information. But it's still plain text because every byte in such a file is there to represent a text character, usually using UTF-8. There are no bytes with nontext meaning, as you can find in a /.png/ or /.xls/ document where most bytes represent packed binary values like RGB values and floating-point numbers. In plain text, numbers are represented as sequences of digit characters.

I am writing this book in a plain-text format called---ironically---[[http://www.methods.co.nz/asciidoc/][AsciiDoc]], which is part of the toolchain of O'Reilly's excellent [[https://atlas.oreilly.com/][Atlas book publishing platform]]. AsciiDoc source files are plain text, but they are UTF-8, not ASCII. Otherwise, writing this chapter would have been really painful. Despite the name, AsciiDoc is just great.

The world of Unicode is constantly expanding and, at the edges, tool support is not always there. That's why I had to use images for Figures [[file:ch04.html#encodings_demo_fig][4-1]], [[file:ch04.html#numerics_demo_fig][4-3]], and [[file:ch04.html#fig_re_demo][4-4]]: not all characters I wanted to show were available in the fonts used to render the book. On the other hand, the Ubuntu 14.04 and OSX 10.9 terminals display them perfectly well---including the Japanese characters for the word “mojibake”: 文字化け.

*Unicode Riddles*

Imprecise qualifiers such as “often,” “most,” and “usually” seem to pop up whenever I write about Unicode normalization. I regret the lack of more definitive advice, but there are so many exceptions to the rules in Unicode that it is hard to be absolutely positive.

For example, the µ (micro sign) is considered a “compatibility character” but the Ω (ohm) and Å (Ångström) symbols are not. The difference has practical consequences: NFC normalization---recommended for text matching---replaces the Ω (ohm) by Ω (uppercase Grek omega) and the Å (Ångström) by Å (uppercase A with ring above). But as a “compatibility character” the µ (micro sign) is not replaced by the visually identical μ (lowercase Greek mu), except when the stronger NFKC or NFKD normalizations are applied, and these transformations are lossy.

I understand the µ (micro sign) is in Unicode because it appears in the =latin1= encoding and replacing it with the Greek mu would break round-trip conversion. After all, that's why the micro sign is a “compatibility character.” But if the ohm and Ångström symbols are not in Unicode for compatibility reasons, then why have them at all? There are already code points for the =GREEK CAPITAL LETTER OMEGA= and the =LATIN CAPITAL LETTER A WITH RING ABOVE=, which look the same and replace them on NFC normalization. Go figure.

My take after many hours studying Unicode: it is hugely complex and full of special cases, reflecting the wonderful variety of human languages and the politics of industry standards.

*How Are str Represented in RAM?*

The official Python docs avoid the issue of how the code points of a =str= are stored in memory. This is, after all, an implementation detail. In theory, it doesn't matter: whatever the internal representation, every =str= must be encoded to =bytes= on output.

In memory, Python 3 stores each =str= as a sequence of code points using a fixed number of bytes per code point, to allow efficient direct access to any character or slice.

Before Python 3.3, CPython could be compiled to use either 16 or 32 bits per code point in RAM; the former was a “narrow build,” and the latter a “wide build.” To know which you have, check the value of =sys.maxunicode=: 65535 implies a “narrow build” that can't handle code points above U+FFFF transparently. A “wide build” doesn't have this limitation, but consumes a lot of memory: 4 bytes per character, even while the vast majority of code points for Chinese ideographs fit in 2 bytes. Neither option was great, so you had to choose depending on your needs.

Since Python 3.3, when creating a new =str= object, the interpreter checks the characters in it and chooses the most economic memory layout that is suitable for that particular =str=: if there are only characters in the =latin1= range, that =str= will use just one byte per code point. Otherwise, 2 or 4 bytes per code point may be used, depending on the =str=. This is a simplification; for the full details, look up [[https://www.python.org/dev/peps/pep-0393/][PEP 393 --- Flexible String Representation]].

The flexible string representation is similar to the way the =int= type works in Python 3: if the integer fits in a machine word, it is stored in one machine word. Otherwise, the interpreter switches to a variable-length representation like that of the Python 2 =long= type. It is nice to see the spread of good ideas.



--------------


^{[[[#id714150][19]]]} Slide 12 of PyCon 2014 talk “Character Encoding and Unicode in Python” ([[http://bit.ly/1JzF1MY][slides]], [[http://bit.ly/1JzF37P][video]]).


^{[[[#id810926][20]]]} [[https://pillow.readthedocs.org/en/latest/][Pillow]] is PIL's most active fork.


^{[[[#id817742][21]]]} As of September, 2014, [[http://bit.ly/w3techs-en][W3Techs: Usage of Character Encodings for Websites]] claims that 81.4% of sites use UTF-8, while [[http://trends.builtwith.com/encoding][Built With: Encoding Usage Statistics]] estimates 79.4%.


^{[[[#id868354][22]]]} I first saw the term “Unicode sandwich” in Ned Batchelder's excellent [[http://nedbatchelder.com/text/unipain/unipain.html][“Pragmatic Unicode” talk]] at US PyCon 2012.


^{[[[#id865027][23]]]} Python 2.6 or 2.7 users have to use =io.open()= to get automatic decoding/encoding when reading/writing.


^{[[[#id721384][24]]]} While researching this subject, I did not find a list of situations when Python 3 internally converts =bytes= to =str=. Python core developer Antoine Pitrou says on the [[http://bit.ly/1IqvSU2][=comp.python.devel= list]] that CPython internal functions that depend on such conversions “don't get a lot of use in py3k.”


^{[[[#id636300][25]]]} The Python 2 =sys.setdefaultencoding= function was misused and is no longer documented in Python 3. It was intended for use by the core developers when the internal default encoding of Python was still undecided. In the same [[http://bit.ly/1IqvN2J][=comp.python.devel= thread]], Marc-André Lemburg states that the =sys.setdefaultencoding= must never be called by user code and the only values supported by CPython are ='ascii'= in Python 2 and ='utf-8'= in Python 3.


^{[[[#id564933][26]]]} Curiously, the micro sign is considered a “compatibility character” but the ohm symbol is not. The end result is that NFC doesn't touch the micro sign but changes the ohm symbol to capital omega, while NFKC and NFKD change both the ohm and the micro into other characters.


^{[[[#id908404][27]]]} Diacritics affect sorting only in the rare case when they are the only difference between two words---in that case, the word with a diacritic is sorted after the plain word.


^{[[[#id410129][28]]]} Thanks to Leonardo Rachael who went beyond his duties as tech reviewer and researched these Windows details, even though he is a GNU/Linux user himself.


^{[[[#id980616][29]]]} Again, I could not find a solution, but did find other people reporting the same problem. Alex Martelli, one of the tech reviewers, had no problem using =setlocale= and =locale.strxfrm= on his Mac with OSX 10.9. In summary: your mileage may vary.


^{[[[#id469746][30]]]} Although it was not better than =re= at identifying digits in this particular sample.


d721384][24]]]} While researching this subject, I did not find a list of situations when Python 3 internally converts =bytes= to =str=. Python core developer Antoine Pitrou says on the [[http://bit.ly/1IqvSU2][=comp.python.devel= list]] that CPython internal functions that depend on such conversions “don't get a lot of use in py3k.”


^{[[[#id636300][25]]]} The Python 2 =sys.setdefaultencoding= function was misused and is no longer documented in Python 3. It was intended for use by the core developers when the internal default encoding of Python was still undecided. In the same [[http://bit.ly/1IqvN2J][=comp.python.devel= thread]], Marc-André Lemburg states that the =sys.setdefaultencoding= must never be called by user code and the only values supported by CPython are ='ascii'= in Python 2 and ='utf-8'= in Python 3.


^{[[[#id564933][26]]]} Curiously, the micro sign is considered a “compatibility character” but the ohm symbol is not. The end result is that NFC doesn't touch the micro sign but changes the ohm symbol to capital omega, while NFKC and NFKD change both the ohm and the micro into other characters.


^{[[[#id908404][27]]]} Diacritics affect sorting only in the rare case when they are the only difference between two words---in that case, the word with a diacritic is sorted after the plain word.


^{[[[#id410129][28]]]} Thanks to Leonardo Rachael who went beyond his duties as tech reviewer and researched these Windows details, even though he is a GNU/Linux user himself.


^{[[[#id980616][29]]]} Again, I could not find a solution, but did find other people reporting the same problem. Alex Martelli, one of the tech reviewers, had no problem using =setlocale= and =locale.strxfrm= on his Mac with OSX 10.9. In summary: your mileage may vary.


^{[[[#id469746][30]]]} Although it was not better than =re= at identifying digits in this particular sample.


[30]]]} Although it was not better than =re= at identifying digits in this particular sample.


e

#+BEGIN_EXAMPLE
    # coding: cp1252

    print('Olá, Mundo!')
#+END_EXAMPLE

*** Tip
    :PROPERTIES:
    :CUSTOM_ID: tip-3
    :CLASS: title
    :END:

Now that Python 3 source code is no longer limited to ASCII and defaults to the excellent UTF-8 encoding, the best “fix” for source code in legacy encodings like ='cp1252'= is to convert them to UTF-8 already, and not bother with the =coding= comments. If your editor does not support UTF-8, it's time to switch.

Non-ASCII Names in Source Code: Should You Use Them?

Python 3 allows non-ASCII identifiers in source code:

#+BEGIN_EXAMPLE
    >>> ação = 'PBR'  # ação = stock
    >>> ε = 10**-6    # ε = epsilon
#+END_EXAMPLE

Some people dislike the idea. The most common argument to stick with ASCII identifiers is to make it easy for everyone to read and edit code. That argument misses the point: you want your source code to be readable and editable by its intended audience, and that may not be “everyone.” If the code belongs to a multinational corporation or is open source and you want contributors from around the world, the identifiers should be in English, and then all you need is ASCII.

But if you are a teacher in Brazil, your students will find it easier to read code that uses Portuguese variable and function names, correctly spelled. And they will have no difficulty typing the cedillas and accented vowels on their localized keyboards.

Now that Python can parse Unicode names and UTF-8 is the default source encoding, I see no point in coding identifiers in Portuguese without accents, as we used to do in Python 2 out of necessity---unless you need the code to run on Python 2 also. If the names are in Portuguese, leaving out the accents won't make the code more readable to anyone.

This is my point of view as a Portuguese-speaking Brazilian, but I believe it applies across borders and cultures: choose the human language that makes the code easier to read by the team, then use the characters needed for correct spelling.

Suppose you have a text file, be it source code or poetry, but you don't know its encoding. How do you detect the actual encoding? The next section answers that with a library recommendation.

*** How to Discover the Encoding of a Byte Sequence
    :PROPERTIES:
    :CUSTOM_ID: discover_encoding
    :CLASS: title
    :END:

How do you find the encoding of a byte sequence? Short answer: you can't. You must be told.

Some communication protocols and file formats, like HTTP and XML, contain headers that explicitly tell us how the content is encoded. You can be sure that some byte streams are not ASCII because they contain byte values over 127, and the way UTF-8 and UTF-16 are built also limits the possible byte sequences. But even then, you can never be 100% positive that a binary file is ASCII or UTF-8 just because certain bit patterns are not there.

However, considering that human languages also have their rules and restrictions, once you assume that a stream of bytes is human /plain text/ it may be possible to sniff out its encoding using heuristics and statistics. For example, if =b'x00'= bytes are common, it is probably a 16- or 32-bit encoding, and not an 8-bit scheme, because null characters in plain text are bugs; when the byte sequence =b'x20x00'= appears often, it is likely to be the space character (U+0020) in a UTF-16LE encoding, rather than the obscure U+2000 =EN QUAD= character---whatever that is.

That is how the package [[https://pypi.python.org/pypi/chardet][Chardet --- The Universal Character Encoding Detector]] works to identify one of 30 supported encodings. Chardet is a Python library that you can use in your programs, but also includes a command-line utility, =chardetect=. Here is what it reports on the source file for this chapter:

#+BEGIN_EXAMPLE
    $ chardetect 04-text-byte.asciidoc
    04-text-byte.asciidoc: utf-8 with confidence 0.99
#+END_EXAMPLE

Although binary sequences of encoded text usually don't carry explicit hints of their encoding, the UTF formats may prepend a byte order mark to the textual content. That is explained next.

*** BOM: A Useful Gremlin
    :PROPERTIES:
    :CUSTOM_ID: _bom_a_useful_gremlin
    :CLASS: title
    :END:

In [[file:ch04.html#ex_codecs][Example 4-5]], you may have noticed a couple of extra bytes at the beginning of a UTF-16 encoded sequence. Here they are again:

#+BEGIN_EXAMPLE
    >>> u16 = 'El Niño'.encode('utf_16')
    >>> u16
    b'xffxfeEx00lx00 x00Nx00ix00xf1x00ox00'
#+END_EXAMPLE

The bytes are =b'xffxfe'=. That is a /BOM/---byte-order mark---denoting the “little-endian” byte ordering of the Intel CPU where the encoding was performed.

On a little-endian machine, for each code point the least significant byte comes first: the letter ='E'=, code point U+0045 (decimal 69), is encoded in byte offsets 2 and 3 as 69 and 0:

#+BEGIN_EXAMPLE
    >>> list(u16)
    [255, 254, 69, 0, 108, 0, 32, 0, 78, 0, 105, 0, 241, 0, 111, 0]
#+END_EXAMPLE

On a big-endian CPU, the encoding would be reversed; ='E'= would be encoded as 0 and 69.

To avoid confusion, the UTF-16 encoding prepends the text to be encoded with the special character =ZERO WIDTH NO-BREAK SPACE= (U+FEFF), which is invisible. On a little-endian system, that is encoded as =b'xffxfe'= (decimal 255, 254). Because, by design, there is no U+FFFE character, the byte sequence =b'xffxfe'= must mean the =ZERO WIDTH NO-BREAK SPACE= on a little-endian encoding, so the codec knows which byte ordering to use.

There is a variant of UTF-16---UTF-16LE---that is explicitly little-endian, and another one explicitly big-endian, UTF-16BE. If you use them, a BOM is not generated:

#+BEGIN_EXAMPLE
    >>> u16le = 'El Niño'.encode('utf_16le')
    >>> list(u16le)
    [69, 0, 108, 0, 32, 0, 78, 0, 105, 0, 241, 0, 111, 0]
    >>> u16be = 'El Niño'.encode('utf_16be')
    >>> list(u16be)
    [0, 69, 0, 108, 0, 32, 0, 78, 0, 105, 0, 241, 0, 111]
#+END_EXAMPLE

If present, the BOM is supposed to be filtered by the UTF-16 codec, so that you only get the actual text contents of the file without the leading =ZERO WIDTH NO-BREAK SPACE=. The standard says that if a file is UTF-16 and has no BOM, it should be assumed to be UTF-16BE (big-endian). However, the Intel x86 architecture is little-endian, so there is plenty of little-endian UTF-16 with no BOM in the wild.

This whole issue of endianness only affects encodings that use words of more than one byte, like UTF-16 and UTF-32. One big advantage of UTF-8 is that it produces the same byte sequence regardless of machine endianness, so no BOM is needed. Nevertheless, some Windows applications (notably Notepad) add the BOM to UTF-8 files anyway---and Excel depends on the BOM to detect a UTF-8 file, otherwise it assumes the content is encoded with a Windows codepage. The character U+FEFF encoded in UTF-8 is the three-byte sequence =b'xefxbbxbf'=. So if a file starts with those three bytes, it is likely to be a UTF-8 file with a BOM. However, Python does not automatically assume a file is UTF-8 just because it starts with =b'xefxbbxbf'=.

We now move on to handling text files in Python 3.

** Handling Text Files


The best practice for handling text is the “Unicode sandwich” ([[file:ch04.html#unicode_sandwich_fig][Figure 4-2]]).^{[[[#ftn.id868354][22]]]} This means that =bytes= should be decoded to =str= as early as possible on input (e.g., when opening a file for reading). The “meat” of the sandwich is the business logic of your program, where text handling is done exclusively on =str= objects. You should never be encoding or decoding in the middle of other processing. On output, the =str= are encoded to =bytes= as late as possible. Most web frameworks work like that, and we rarely touch =bytes= when using them. In Django, for example, your views should output Unicode =str=; Django itself takes care of encoding the response to =bytes=, using UTF-8 by default.



[[file:images/flup_0402.png.jpg]]

Figure 4-2. Unicode sandwich: current best practice for text processing

Python 3 makes it easier to follow the advice of the Unicode sandwich, because the =open= built-in does the necessary decoding when reading and encoding when writing files in text mode, so all you get from =my_file.read()= and pass to =my_file.write(text)= are =str= objects.^{[[[#ftn.id865027][23]]]}

Therefore, using text files is simple. But if you rely on default encodings you will get bitten.

Consider the console session in [[file:ch04.html#ex_cafe_file1][Example 4-9]]. Can you spot the bug?



Example 4-9. A platform encoding issue (if you try this on your machine, you may or may not see the problem)

#+BEGIN_EXAMPLE
    >>> open('cafe.txt', 'w', encoding='utf_8').write('café')
    4
    >>> open('cafe.txt').read()
    'cafÃ©'
#+END_EXAMPLE

The bug: I specified UTF-8 encoding when writing the file but failed to do so when reading it, so Python assumed the system default encoding---Windows 1252---and the trailing bytes in the file were decoded as characters ='Ã©'= instead of ='é'=.

I ran [[file:ch04.html#ex_cafe_file1][Example 4-9]] on a Windows 7 machine. The same statements running on recent GNU/Linux or Mac OSX work perfectly well because their default encoding is UTF-8, giving the false impression that everything is fine. If the encoding argument was omitted when opening the file to write, the locale default encoding would be used, and we'd read the file correctly using the same encoding. But then this script would generate files with different byte contents depending on the platform or even depending on locale settings in the same platform, creating compatibility problems.

*** Tip
    :PROPERTIES:
    :CUSTOM_ID: tip-4
    :CLASS: title
    :END:

Code that has to run on multiple machines or on multiple occasions should never depend on encoding defaults. Always pass an explicit =encoding== argument when opening text files, because the default may change from one machine to the next, or from one day to the next.

A curious detail in [[file:ch04.html#ex_cafe_file1][Example 4-9]] is that the =write= function in the first statement reports that four characters were written, but in the next line five characters are read. [[file:ch04.html#ex_cafe_file2][Example 4-10]] is an extended version of [[file:ch04.html#ex_cafe_file1][Example 4-9]], explaining that and other details.



Example 4-10. Closer inspection of [[file:ch04.html#ex_cafe_file1][Example 4-9]] running on Windows reveals the bug and how to fix it

#+BEGIN_EXAMPLE
    >>> fp = open('cafe.txt', 'w', encoding='utf_8')
    >>> fp  
    <_io.TextIOWrapper name='cafe.txt' mode='w' encoding='utf_8'>
    >>> fp.write('café')
    4  
    >>> fp.close()
    >>> import os
    >>> os.stat('cafe.txt').st_size
    5  
    >>> fp2 = open('cafe.txt')
    >>> fp2  
    <_io.TextIOWrapper name='cafe.txt' mode='r' encoding='cp1252'>
    >>> fp2.encoding  
    'cp1252'
    >>> fp2.read()
    'cafÃ©'  
    >>> fp3 = open('cafe.txt', encoding='utf_8')  
    >>> fp3
    <_io.TextIOWrapper name='cafe.txt' mode='r' encoding='utf_8'>
    >>> fp3.read()
    'café'  
    >>> fp4 = open('cafe.txt', 'rb')  
    >>> fp4
    <_io.BufferedReader name='cafe.txt'>  
    >>> fp4.read()  
    b'cafxc3xa9'
#+END_EXAMPLE

- [[#CO39-1][[[file:callouts/1.png]]]]  :: By default, =open= operates in text mode and returns a =TextIOWrapper= object.

- [[#CO39-2][[[file:callouts/2.png]]]]  :: The =write= method on a =TextIOWrapper= returns the number of Unicode characters written.

- [[#CO39-3][[[file:callouts/3.png]]]]  :: =os.stat= reports that the file holds 5 bytes; UTF-8 encodes ='é'= as 2 bytes, 0xc3 and 0xa9.

- [[#CO39-4][[[file:callouts/4.png]]]]  :: Opening a text file with no explicit encoding returns a =TextIOWrapper= with the encoding set to a default from the locale.

- [[#CO39-5][[[file:callouts/5.png]]]]  :: A =TextIOWrapper= object has an encoding attribute that you can inspect: =cp1252= in this case.

- [[#CO39-6][[[file:callouts/6.png]]]]  :: In the Windows =cp1252= encoding, the byte 0xc3 is an “Ã” (A with tilde) and 0xa9 is the copyright sign.

- [[#CO39-7][[[file:callouts/7.png]]]]  :: Opening the same file with the correct encoding.

- [[#CO39-8][[[file:callouts/8.png]]]]  :: The expected result: the same four Unicode characters for ='café'=.

- [[#CO39-9][[[file:callouts/9.png]]]]  :: The ='rb'= flag opens a file for reading in binary mode.

- [[#CO39-10][[[file:callouts/10.png]]]]  :: The returned object is a =BufferedReader= and not a =TextIOWrapper=.

- [[#CO39-11][[[file:callouts/11.png]]]]  :: Reading that returns bytes, as expected.

*** Tip
    :PROPERTIES:
    :CUSTOM_ID: tip-5
    :CLASS: title
    :END:

Do not open text files in binary mode unless you need to analyze the file contents to determine the encoding---even then, you should be using Chardet instead of reinventing the wheel (see [[file:ch04.html#discover_encoding][How to Discover the Encoding of a Byte Sequence]]). Ordinary code should only use binary mode to open binary files, like raster images.

The problem in [[file:ch04.html#ex_cafe_file2][Example 4-10]] has to do with relying on a default setting while opening a text file. There are several sources for such defaults, as the next section shows.

*** Encoding Defaults: A Madhouse
    :PROPERTIES:
    :CUSTOM_ID: _encoding_defaults_a_madhouse
    :CLASS: title
    :END:

Several settings affect the encoding defaults for I/O in Python. See the /default_encodings.py/ script in [[file:ch04.html#ex_default_encodings][Example 4-11]].



Example 4-11. Exploring encoding defaults

#+BEGIN_EXAMPLE
    import sys, locale

    expressions = """
            locale.getpreferredencoding()
            type(my_file)
            my_file.encoding
            sys.stdout.isatty()
            sys.stdout.encoding
            sys.stdin.isatty()
            sys.stdin.encoding
            sys.stderr.isatty()
            sys.stderr.encoding
            sys.getdefaultencoding()
            sys.getfilesystemencoding()
        """

    my_file = open('dummy', 'w')

    for expression in expressions.split():
        value = eval(expression)
        print(expression.rjust(30), '->', repr(value))
#+END_EXAMPLE

The output of [[file:ch04.html#ex_default_encodings][Example 4-11]] on GNU/Linux (Ubuntu 14.04) and OSX (Mavericks 10.9) is identical, showing that =UTF-8= is used everywhere in these systems:

#+BEGIN_EXAMPLE
    $ python3 default_encodings.py
     locale.getpreferredencoding() -> 'UTF-8'
                     type(my_file) -> <class '_io.TextIOWrapper'>
                  my_file.encoding -> 'UTF-8'
               sys.stdout.isatty() -> True
               sys.stdout.encoding -> 'UTF-8'
                sys.stdin.isatty() -> True
                sys.stdin.encoding -> 'UTF-8'
               sys.stderr.isatty() -> True
               sys.stderr.encoding -> 'UTF-8'
          sys.getdefaultencoding() -> 'utf-8'
       sys.getfilesystemencoding() -> 'utf-8'
#+END_EXAMPLE

On Windows, however, the output is [[file:ch04.html#ex_default_encodings_cmd][Example 4-12]].



Example 4-12. Default encodings on Windows 7 (SP 1) cmd.exe localized for Brazil; PowerShell gives same result

#+BEGIN_EXAMPLE
    Z:>chcp  
    Página de código ativa: 850
    Z:>python default_encodings.py  
     locale.getpreferredencoding() -> 'cp1252'  
                     type(my_file) -> <class '_io.TextIOWrapper'>
                  my_file.encoding -> 'cp1252'  
               sys.stdout.isatty() -> True      
               sys.stdout.encoding -> 'cp850'   
                sys.stdin.isatty() -> True
                sys.stdin.encoding -> 'cp850'
               sys.stderr.isatty() -> True
               sys.stderr.encoding -> 'cp850'
          sys.getdefaultencoding() -> 'utf-8'
       sys.getfilesystemencoding() -> 'mbcs'
#+END_EXAMPLE

- [[#CO40-1][[[file:callouts/1.png]]]]  :: =chcp= shows the active codepage for the console: 850.

- [[#CO40-2][[[file:callouts/2.png]]]]  :: Running /default_encodings.py/ with output to console.

- [[#CO40-3][[[file:callouts/3.png]]]]  :: =locale.getpreferredencoding()= is the most important setting.

- [[#CO40-4][[[file:callouts/4.png]]]]  :: Text files use =locale.getpreferredencoding()= by default.

- [[#CO40-5][[[file:callouts/5.png]]]]  :: The output is going to the console, so =sys.stdout.isatty()= is =True=.

- [[#CO40-6][[[file:callouts/6.png]]]]  :: Therefore, =sys.stdout.encoding= is the same as the console encoding.

If the output is redirected to a file, like this:

#+BEGIN_EXAMPLE
    Z:>python default_encodings.py > encodings.log
#+END_EXAMPLE

The value of =sys.stdout.isatty()= becomes =False=, and =sys.stdout.encoding= is set by =locale.getpreferredencoding()=, ='cp1252'= in that machine.

Note that there are four different encodings in [[file:ch04.html#ex_default_encodings_cmd][Example 4-12]]:

- If you omit the =encoding= argument when opening a file, the default is given by =locale.getpreferredencoding()= (='cp1252'= in [[file:ch04.html#ex_default_encodings_cmd][Example 4-12]]).
- The encoding of =sys.stdout/stdin/stderr= is given by the [[http://bit.ly/1IqvCUZ][=PYTHONIOENCODING= environment variable]], if present, otherwise it is either inherited from the console or defined by =locale.getpreferredencoding()= if the output/input is redirected to/from a file.
- =sys.getdefaultencoding()= is used internally by Python to convert binary data to/from =str=; this happens less often in Python 3, but still happens.^{[[[#ftn.id721384][24]]]} Changing this setting is not supported.^{[[[#ftn.id636300][25]]]}
- =sys.getfilesystemencoding()= is used to encode/decode filenames (not file contents). It is used when =open()= gets a =str= argument for the filename; if the filename is given as a =bytes= argument, it is passed unchanged to the OS API. The Python [[https://docs.python.org/3/howto/unicode.html][Unicode HOWTO]] says: “on Windows, Python uses the name =mbcs= to refer to whatever the currently configured encoding is.” The acronym MBCS stands for Multi Byte Character Set, which for Microsoft are the legacy variable-width encodings like =gb2312= or =Shift_JIS=, but not UTF-8. (On this topic, a useful answer on StackOverflow is [[http://bit.ly/1IqvRPV][“Difference between MBCS and UTF-8 on Windows”]].)

*** Note
    :PROPERTIES:
    :CUSTOM_ID: note-2
    :CLASS: title
    :END:

On GNU/Linux and OSX all of these encodings are set to UTF-8 by default, and have been for several years, so I/O handles all Unicode characters. On Windows, not only are different encodings used in the same system, but they are usually codepages like ='cp850'= or ='cp1252'= that support only ASCII with 127 additional characters that are not the same from one encoding to the other. Therefore, Windows users are far more likely to face encoding errors unless they are extra careful.

To summarize, the most important encoding setting is that returned by =locale.getpreferredencoding()=: it is the default for opening text files and for =sys.stdout/stdin/stderr= when they are redirected to files. However, the [[http://bit.ly/1IqvYLp][documentation]] reads (in part):

#+BEGIN_QUOTE

  -  =locale.getpreferredencoding(do_setlocale=True)=  :: Return the encoding used for text data, according to user preferences. User preferences are expressed differently on different systems, and might not be available programmatically on some systems, so this function only returns a guess. [...]

#+END_QUOTE

Therefore, the best advice about encoding defaults is: do not rely on them.

If you follow the advice of the Unicode sandwich and always are explicit about the encodings in your programs, you will avoid a lot of pain. Unfortunately, Unicode is painful even if you get your =bytes= correctly converted to =str=. The next two sections cover subjects that are simple in ASCII-land, but get quite complex on planet Unicode: text normalization (i.e., converting text to a uniform representation for comparisons) and sorting.

** Normalizing Unicode for Saner Comparisons


String comparisons are complicated by the fact that Unicode has combining characters: diacritics and other marks that attach to the preceding character, appearing as one when printed.

For example, the word “café” may be composed in two ways, using four or five code points, but the result looks exactly the same:

#+BEGIN_EXAMPLE
    >>> s1 = 'café'
    >>> s2 = 'cafeu0301'
    >>> s1, s2
    ('café', 'café')
    >>> len(s1), len(s2)
    (4, 5)
    >>> s1 == s2
    False
#+END_EXAMPLE

The code point U+0301 is the =COMBINING ACUTE ACCENT=. Using it after “e” renders “é”. In the Unicode standard, sequences like ='é'= and ='eu0301'= are called “canonical equivalents,” and applications are supposed to treat them as the same. But Python sees two different sequences of code points, and considers them not equal.

The solution is to use Unicode normalization, provided by the =unicodedata.normalize= function. The first argument to that function is one of four strings: ='NFC'=, ='NFD'=, ='NFKC'=, and ='NFKD'=. Let's start with the first two.

Normalization Form C (NFC) composes the code points to produce the shortest equivalent string, while NFD decomposes, expanding composed characters into base characters and separate combining characters. Both of these normalizations make comparisons work as expected:

#+BEGIN_EXAMPLE
    >>> from unicodedata import normalize
    >>> s1 = 'café'  # composed "e" with acute accent
    >>> s2 = 'cafeu0301'  # decomposed "e" and acute accent
    >>> len(s1), len(s2)
    (4, 5)
    >>> len(normalize('NFC', s1)), len(normalize('NFC', s2))
    (4, 4)
    >>> len(normalize('NFD', s1)), len(normalize('NFD', s2))
    (5, 5)
    >>> normalize('NFC', s1) == normalize('NFC', s2)
    True
    >>> normalize('NFD', s1) == normalize('NFD', s2)
    True
#+END_EXAMPLE

Western keyboards usually generate composed characters, so text typed by users will be in NFC by default. However, to be safe, it may be good to sanitize strings with =normalize('NFC', user_text)= before saving. NFC is also the normalization form recommended by the W3C in [[http://www.w3.org/TR/charmod-norm/][Character Model for the World Wide Web: String Matching and Searching]].

Some single characters are normalized by NFC into another single character. The symbol for the ohm (Ω) unit of electrical resistance is normalized to the Greek uppercase omega. They are visually identical, but they compare unequal so it is essential to normalize to avoid surprises:

#+BEGIN_EXAMPLE
    >>> from unicodedata import normalize, name
    >>> ohm = 'u2126'
    >>> name(ohm)
    'OHM SIGN'
    >>> ohm_c = normalize('NFC', ohm)
    >>> name(ohm_c)
    'GREEK CAPITAL LETTER OMEGA'
    >>> ohm == ohm_c
    False
    >>> normalize('NFC', ohm) == normalize('NFC', ohm_c)
    True
#+END_EXAMPLE

In the acronyms for the other two normalization forms---NFKC and NFKD---the letter K stands for “compatibility.” These are stronger forms of normalization, affecting the so-called “compatibility characters.” Although one goal of Unicode is to have a single “canonical” code point for each character, some characters appear more than once for compatibility with preexisting standards. For example, the micro sign, ='µ'= (=U+00B5=), was added to Unicode to support round-trip conversion to =latin1=, even though the same character is part of the Greek alphabet with code point =U+03BC= (=GREEK SMALL LETTER MU=). So, the micro sign is considered a “compatibility character.”

In the NFKC and NFKD forms, each compatibility character is replaced by a “compatibility decomposition” of one or more characters that are considered a “preferred” representation, even if there is some formatting loss---ideally, the formatting should be the responsibility of external markup, not part of Unicode. To exemplify, the compatibility decomposition of the one half fraction ='½'= (=U+00BD=) is the sequence of three characters ='1/2'=, and the compatibility decomposition of the micro sign ='µ'= (=U+00B5=) is the lowercase mu ='μ'= (=U+03BC=).^{[[[#ftn.id564933][26]]]}

Here is how the NFKC works in practice:

#+BEGIN_EXAMPLE
    >>> from unicodedata import normalize, name
    >>> half = '½'
    >>> normalize('NFKC', half)
    '1⁄2'
    >>> four_squared = '4²'
    >>> normalize('NFKC', four_squared)
    '42'
    >>> micro = 'µ'
    >>> micro_kc = normalize('NFKC', micro)
    >>> micro, micro_kc
    ('µ', 'μ')
    >>> ord(micro), ord(micro_kc)
    (181, 956)
    >>> name(micro), name(micro_kc)
    ('MICRO SIGN', 'GREEK SMALL LETTER MU')
#+END_EXAMPLE

Although ='1⁄2'= is a reasonable substitute for ='½'=, and the micro sign is really a lowercase Greek mu, converting ='4²'= to ='42'= changes the meaning. An application could store ='4²'= as ='4<sup>2</sup>'=, but the =normalize= function knows nothing about formatting. Therefore, NFKC or NFKD may lose or distort information, but they can produce convenient intermediate representations for searching and indexing: users may be pleased that a search for ='1⁄2 inch'= also finds documents containing ='½ inch'=.

*** Warning
    :PROPERTIES:
    :CUSTOM_ID: warning-1
    :CLASS: title
    :END:

NFKC and NFKD normalization should be applied with care and only in special cases---e.g., search and indexing---and not for permanent storage, because these transformations cause data loss.

When preparing text for searching or indexing, another operation is useful: case folding, our next subject.

*** Case Folding
    :PROPERTIES:
    :CUSTOM_ID: _case_folding
    :CLASS: title
    :END:

Case folding is essentially converting all text to lowercase, with some additional transformations. It is supported by the =str.casefold()= method (new in Python 3.3).

For any string =s= containing only =latin1= characters, =s.casefold()= produces the same result as =s.lower()=, with only two exceptions---the micro sign ='µ'= is changed to the Greek lowercase mu (which looks the same in most fonts) and the German Eszett or “sharp s” (ß) becomes “ss”:

#+BEGIN_EXAMPLE
    >>> micro = 'µ'
    >>> name(micro)
    'MICRO SIGN'
    >>> micro_cf = micro.casefold()
    >>> name(micro_cf)
    'GREEK SMALL LETTER MU'
    >>> micro, micro_cf
    ('µ', 'μ')
    >>> eszett = 'ß'
    >>> name(eszett)
    'LATIN SMALL LETTER SHARP S'
    >>> eszett_cf = eszett.casefold()
    >>> eszett, eszett_cf
    ('ß', 'ss')
#+END_EXAMPLE

As of Python 3.4, there are 116 code points for which =str.casefold()= and =str.lower()= return different results. That's 0.11% of a total of 110,122 named characters in Unicode 6.3.

As usual with anything related to Unicode, case folding is a complicated issue with plenty of linguistic special cases, but the Python core team made an effort to provide a solution that hopefully works for most users.

In the next couple of sections, we'll put our normalization knowledge to use developing utility functions.

*** Utility Functions for Normalized Text Matching
    :PROPERTIES:
    :CUSTOM_ID: _utility_functions_for_normalized_text_matching
    :CLASS: title
    :END:

As we've seen, NFC and NFD are safe to use and allow sensible comparisons between Unicode strings. NFC is the best normalized form for most applications. =str.casefold()= is the way to go for case-insensitive comparisons.

If you work with text in many languages, a pair of functions like =nfc_equal= and =fold_equal= in [[file:ch04.html#ex_normeq][Example 4-13]] are useful additions to your toolbox.



Example 4-13. normeq.py: normalized Unicode string comparison

#+BEGIN_EXAMPLE
    """
    Utility functions for normalized Unicode string comparison.

    Using Normal Form C, case sensitive:

        >>> s1 = 'café'
        >>> s2 = 'cafeu0301'
        >>> s1 == s2
        False
        >>> nfc_equal(s1, s2)
        True
        >>> nfc_equal('A', 'a')
        False

    Using Normal Form C with case folding:

        >>> s3 = 'Straße'
        >>> s4 = 'strasse'
        >>> s3 == s4
        False
        >>> nfc_equal(s3, s4)
        False
        >>> fold_equal(s3, s4)
        True
        >>> fold_equal(s1, s2)
        True
        >>> fold_equal('A', 'a')
        True

    """

    from unicodedata import normalize

    def nfc_equal(str1, str2):
        return normalize('NFC', str1) == normalize('NFC', str2)

    def fold_equal(str1, str2):
        return (normalize('NFC', str1).casefold() ==
                normalize('NFC', str2).casefold())
#+END_EXAMPLE

Beyond Unicode normalization and case folding---which are both part of the Unicode standard---sometimes it makes sense to apply deeper transformations, like changing ='café'= into ='cafe'=. We'll see when and how in the next section.

*** Extreme “Normalization”: Taking Out Diacritics
    :PROPERTIES:
    :CUSTOM_ID: _extreme_normalization_taking_out_diacritics
    :CLASS: title
    :END:

The Google Search secret sauce involves many tricks, but one of them apparently is ignoring diacritics (e.g., accents, cedillas, etc.), at least in some contexts. Removing diacritics is not a proper form of normalization because it often changes the meaning of words and may produce false positives when searching. But it helps coping with some facts of life: people sometimes are lazy or ignorant about the correct use of diacritics, and spelling rules change over time, meaning that accents come and go in living languages.

Outside of searching, getting rid of diacritics also makes for more readable URLs, at least in Latin-based languages. Take a look at the URL for the Wikipedia article about the city of São Paulo:

#+BEGIN_SRC screen
    http://en.wikipedia.org/wiki/S%C3%A3o_Paulo
#+END_SRC

The /%C3%A3/ part is the URL-escaped, UTF-8 rendering of the single letter “ã” (“a” with tilde). The following is much friendlier, even if it is not the right spelling:

#+BEGIN_SRC screen
    http://en.wikipedia.org/wiki/Sao_Paulo
#+END_SRC

To remove all diacritics from a =str=, you can use a function like [[file:ch04.html#ex_shave_marks][Example 4-14]].



Example 4-14. Function to remove all combining marks (module sanitize.py)

#+BEGIN_EXAMPLE
    import unicodedata
    import string


    def shave_marks(txt):
        """Remove all diacritic marks"""
        norm_txt = unicodedata.normalize('NFD', txt)   
        shaved = ''.join(c for c in norm_txt
                         if not unicodedata.combining(c))   
        return unicodedata.normalize('NFC', shaved)   
#+END_EXAMPLE

- [[#CO41-1][[[file:callouts/1.png]]]]  :: Decompose all characters into base characters and combining marks.

- [[#CO41-2][[[file:callouts/2.png]]]]  :: Filter out all combining marks.

- [[#CO41-3][[[file:callouts/3.png]]]]  :: Recompose all characters.

[[file:ch04.html#ex_shave_marks_demo][Example 4-15]] shows a couple of uses of =shave_marks=.



Example 4-15. Two examples using shave_marks from [[file:ch04.html#ex_shave_marks][Example 4-14]]

#+BEGIN_EXAMPLE
    >>> order = '“Herr Voß: • ½ cup of Œtker™ caffè latte • bowl of açaí.”'
    >>> shave_marks(order)
    '“Herr Voß: • ½ cup of Œtker™ caffe latte • bowl of acai.”'  
    >>> Greek = 'Ζέφυρος, Zéfiro'
    >>> shave_marks(Greek)
    'Ζεφυρος, Zefiro'  
#+END_EXAMPLE

- [[#CO42-1][[[file:callouts/1.png]]]]  :: Only the letters “è”, “ç”, and “í” were replaced.

- [[#CO42-2][[[file:callouts/2.png]]]]  :: Both “έ” and “é” were replaced.

The function =shave_marks= from [[file:ch04.html#ex_shave_marks][Example 4-14]] works all right, but maybe it goes too far. Often the reason to remove diacritics is to change Latin text to pure ASCII, but =shave_marks= also changes non-Latin characters---like Greek letters---which will never become ASCII just by losing their accents. So it makes sense to analyze each base character and to remove attached marks only if the base character is a letter from the Latin alphabet. This is what [[file:ch04.html#ex_shave_marks_latin][Example 4-16]] does.



Example 4-16. Function to remove combining marks from Latin characters (import statements are omitted as this is part of the sanitize.py module from [[file:ch04.html#ex_shave_marks][Example 4-14]])

#+BEGIN_EXAMPLE
    def shave_marks_latin(txt):
        """Remove all diacritic marks from Latin base characters"""
        norm_txt = unicodedata.normalize('NFD', txt)   
        latin_base = False
        keepers = []
        for c in norm_txt:
            if unicodedata.combining(c) and latin_base:    
                continue  # ignore diacritic on Latin base char
            keepers.append(c)                              
            # if it isn't combining char, it's a new base char
            if not unicodedata.combining(c):               
                latin_base = c in string.ascii_letters
        shaved = ''.join(keepers)
        return unicodedata.normalize('NFC', shaved)    
#+END_EXAMPLE

- [[#CO43-1][[[file:callouts/1.png]]]]  :: Decompose all characters into base characters and combining marks.

- [[#CO43-2][[[file:callouts/2.png]]]]  :: Skip over combining marks when base character is Latin.

- [[#CO43-3][[[file:callouts/3.png]]]]  :: Otherwise, keep current character.

- [[#CO43-4][[[file:callouts/4.png]]]]  :: Detect new base character and determine if it's Latin.

- [[#CO43-5][[[file:callouts/5.png]]]]  :: Recompose all characters.

An even more radical step would be to replace common symbols in Western texts (e.g., curly quotes, em dashes, bullets, etc.) into =ASCII= equivalents. This is what the function =asciize= does in [[file:ch04.html#ex_asciize][Example 4-17]].



Example 4-17. Transform some Western typographical symbols into ASCII (this snippet is also part of sanitize.py from [[file:ch04.html#ex_shave_marks][Example 4-14]])

#+BEGIN_EXAMPLE
    single_map = str.maketrans("""‚ƒ„†ˆ‹‘’“”•–—˜›""",   
                               """'f"*^<''""---~>""")

    multi_map = str.maketrans({   
        '€': '<euro>',
        '…': '...',
        'Œ': 'OE',
        '™': '(TM)',
        'œ': 'oe',
        '‰': '<per mille>',
        '‡': '**',
    })

    multi_map.update(single_map)   


    def dewinize(txt):
        """Replace Win1252 symbols with ASCII chars or sequences"""
        return txt.translate(multi_map)   


    def asciize(txt):
        no_marks = shave_marks_latin(dewinize(txt))      
        no_marks = no_marks.replace('ß', 'ss')           
        return unicodedata.normalize('NFKC', no_marks)   
#+END_EXAMPLE

- [[#CO44-1][[[file:callouts/1.png]]]]  :: Build mapping table for char-to-char replacement.

- [[#CO44-2][[[file:callouts/2.png]]]]  :: Build mapping table for char-to-string replacement.

- [[#CO44-3][[[file:callouts/3.png]]]]  :: Merge mapping tables.

- [[#CO44-4][[[file:callouts/4.png]]]]  :: =dewinize= does not affect =ASCII= or =latin1= text, only the Microsoft additions in to =latin1= in =cp1252=.

- [[#CO44-5][[[file:callouts/5.png]]]]  :: Apply =dewinize= and remove diacritical marks.

- [[#CO44-6][[[file:callouts/6.png]]]]  :: Replace the Eszett with “ss” (we are not using case fold here because we want to preserve the case).

- [[#CO44-7][[[file:callouts/7.png]]]]  :: Apply NFKC normalization to compose characters with their compatibility code points.

[[file:ch04.html#ex_asciize_demo][Example 4-18]] shows =asciize= in use.



Example 4-18. Two examples using asciize from [[file:ch04.html#ex_asciize][Example 4-17]]

#+BEGIN_EXAMPLE
    >>> order = '“Herr Voß: • ½ cup of Œtker™ caffè latte • bowl of açaí.”'
    >>> dewinize(order)
    '"Herr Voß: - ½ cup of OEtker(TM) caffè latte - bowl of açaí."'  
    >>> asciize(order)
    '"Herr Voss: - 1⁄2 cup of OEtker(TM) caffe latte - bowl of acai."'  
#+END_EXAMPLE

- [[#CO45-1][[[file:callouts/1.png]]]]  :: =dewinize= replaces curly quotes, bullets, and ™ (trademark symbol).

- [[#CO45-2][[[file:callouts/2.png]]]]  :: =asciize= applies =dewinize=, drops diacritics, and replaces the ='ß'=.

*** Warning
    :PROPERTIES:
    :CUSTOM_ID: warning-2
    :CLASS: title
    :END:

Different languages have their own rules for removing diacritics. For example, Germans change the ='ü'= into ='ue'=. Our =asciize= function is not as refined, so it may or not be suitable for your language. It works acceptably for Portuguese, though.

To summarize, the functions in /sanitize.py/ go way beyond standard normalization and perform deep surgery on the text, with a good chance of changing its meaning. Only you can decide whether to go so far, knowing the target language, your users, and how the transformed text will be used.

This wraps up our discussion of normalizing Unicode text.

The next Unicode matter to sort out is... sorting.

** Sorting Unicode Text


Python sorts sequences of any type by comparing the items in each sequence one by one. For strings, this means comparing the code points. Unfortunately, this produces unacceptable results for anyone who uses non-ASCII characters.

Consider sorting a list of fruits grown in Brazil:

#+BEGIN_EXAMPLE
    >>> fruits = ['caju', 'atemoia', 'cajá', 'açaí', 'acerola']
    >>> sorted(fruits)
    ['acerola', 'atemoia', 'açaí', 'caju', 'cajá']
#+END_EXAMPLE

Sorting rules vary for different locales, but in Portuguese and many languages that use the Latin alphabet, accents and cedillas rarely make a difference when sorting.^{[[[#ftn.id908404][27]]]} So “cajá” is sorted as “caja,” and must come before “caju.”

The sorted =fruits= list should be:

#+BEGIN_EXAMPLE
    ['açaí', 'acerola', 'atemoia', 'cajá', 'caju']
#+END_EXAMPLE

The standard way to sort non-ASCII text in Python is to use the =locale.strxfrm= function which, according to the [[http://bit.ly/1IqyCRf][=locale= module docs]], “transforms a string to one that can be used in locale-aware comparisons.”

To enable =locale.strxfrm=, you must first set a suitable locale for your application, and pray that the OS supports it. On GNU/Linux (Ubuntu 14.04) with the =pt_BR= locale, the sequence of commands in [[file:ch04.html#ex_locale_sort][Example 4-19]] works.



Example 4-19. Using the locale.strxfrm function as sort key

#+BEGIN_EXAMPLE
    >>> import locale
    >>> locale.setlocale(locale.LC_COLLATE, 'pt_BR.UTF-8')
    'pt_BR.UTF-8'
    >>> fruits = ['caju', 'atemoia', 'cajá', 'açaí', 'acerola']
    >>> sorted_fruits = sorted(fruits, key=locale.strxfrm)
    >>> sorted_fruits
    ['açaí', 'acerola', 'atemoia', 'cajá', 'caju']
#+END_EXAMPLE

So you need to call =setlocale(LC_COLLATE, «your_locale»)= before using =locale.strxfrm= as the key when sorting.

There are a few caveats, though:

- Because locale settings are global, calling =setlocale= in a library is not recommended. Your application or framework should set the locale when the process starts, and should not change it afterwards.
- The locale must be installed on the OS, otherwise =setlocale= raises a =locale.Error: unsupported locale setting= exception.
- You must know how to spell the locale name. They are pretty much standardized in the Unix derivatives as ='language_code.encoding'=, but on Windows the syntax is more complicated: =Language Name-Language Variant_Region Name.codepage>=. Note that the Language Name, Language Variant, and Region Name parts can have spaces inside them, but the parts after the first are prefixed with special different characters: a hyphen, an underline character, and a dot. All parts seem to be optional except the language name. For example: =English_United States.850= means Language Name “English”, region “United States”, and codepage “850”. The language and region names Windows understands are listed in the MSDN article [[http://bit.ly/1IqyKAl][Language Identifier Constants and Strings]], while [[http://bit.ly/1IqyP79][Code Page Identifiers]] lists the numbers for the last part.^{[[[#ftn.id410129][28]]]}
- The locale must be correctly implemented by the makers of the OS. I was successful on Ubuntu 14.04, but not on OSX (Mavericks 10.9). On two different Macs, the call =setlocale(LC_COLLATE, 'pt_BR.UTF-8')= returns the string ='pt_BR.UTF-8'= with no complaints. But =sorted(fruits, key=locale.strxfrm)= produced the same incorrect result as =sorted(fruits)= did. I also tried the =fr_FR=, =es_ES=, and =de_DE= locales on OSX, but =locale.strxfrm= never did its job.^{[[[#ftn.id980616][29]]]}

So the standard library solution to internationalized sorting works, but seems to be well supported only on GNU/Linux (perhaps also on Windows, if you are an expert). Even then, it depends on locale settings, creating deployment headaches.

Fortunately, there is a simpler solution: the PyUCA library, available on /PyPI/.

*** Sorting with the Unicode Collation Algorithm
    :PROPERTIES:
    :CUSTOM_ID: _sorting_with_the_unicode_collation_algorithm
    :CLASS: title
    :END:

James Tauber, prolific Django contributor, must have felt the pain and created [[https://pypi.python.org/pypi/pyuca/][PyUCA]], a pure-Python implementation of the Unicode Collation Algorithm (UCA). [[file:ch04.html#ex_pyuca_sort][Example 4-20]] shows how easy it is to use.



Example 4-20. Using the pyuca.Collator.sort_key method

#+BEGIN_EXAMPLE
    >>> import pyuca
    >>> coll = pyuca.Collator()
    >>> fruits = ['caju', 'atemoia', 'cajá', 'açaí', 'acerola']
    >>> sorted_fruits = sorted(fruits, key=coll.sort_key)
    >>> sorted_fruits
    ['açaí', 'acerola', 'atemoia', 'cajá', 'caju']
#+END_EXAMPLE

This is friendly and just works. I tested it on GNU/Linux, OSX, and Windows. Only Python 3.X is supported at this time.

PyUCA does not take the locale into account. If you need to customize the sorting, you can provide the path to a custom collation table to the =Collator()= constructor. Out of the box, it uses [[https://github.com/jtauber/pyuca][=allkeys.txt=]], which is bundled with the project. That's just a copy of the [[http://bit.ly/1IqAk54][Default Unicode Collation Element Table]] from Unicode 6.3.0.

By the way, that table is one of the many that comprise the Unicode database, our next subject.

** The Unicode Database


The Unicode standard provides an entire database---in the form of numerous structured text files---that includes not only the table mapping code points to character names, but also metadata about the individual characters and how they are related. For example, the Unicode database records whether a character is printable, is a letter, is a decimal digit, or is some other numeric symbol. That's how the =str= methods =isidentifier=, =isprintable=, =isdecimal=, and =isnumeric= work. =str.casefold= also uses information from a Unicode table.

The =unicodedata= module has functions that return character metadata; for instance, its official name in the standard, whether it is a combining character (e.g., diacritic like a combining tilde), and the numeric value of the symbol for humans (not its code point). [[file:ch04.html#ex_numerics_demo][Example 4-21]] shows the use of =unicodedata.name()= and =unicodedata.numeric()= along with the =.isdecimal()= and =.isnumeric()= methods of =str=.



Example 4-21. Demo of Unicode database numerical character metadata (callouts describe each column in the output)

#+BEGIN_EXAMPLE
    import unicodedata
    import re

    re_digit = re.compile(r'd')

    sample = '1xbcxb2u0969u136bu216bu2466u2480u3285'

    for char in sample:
        print('U+%04x' % ord(char),                        
              char.center(6),                              
              're_dig' if re_digit.match(char) else '-',   
              'isdig' if char.isdigit() else '-',          
              'isnum' if char.isnumeric() else '-',        
              format(unicodedata.numeric(char), '5.2f'),   
              unicodedata.name(char),                      
              sep='t')
#+END_EXAMPLE

- [[#CO46-1][[[file:callouts/1.png]]]]  :: Code point in =U+0000= format.

- [[#CO46-2][[[file:callouts/2.png]]]]  :: Character centralized in a =str= of length 6.

- [[#CO46-3][[[file:callouts/3.png]]]]  :: Show =re_dig= if character matches the =r'd'= regex.

- [[#CO46-4][[[file:callouts/4.png]]]]  :: Show =isdig= if =char.isdigit()= is =True=.

- [[#CO46-5][[[file:callouts/5.png]]]]  :: Show =isnum= if =char.isnumeric()= is =True=.

- [[#CO46-6][[[file:callouts/6.png]]]]  :: Numeric value formated with width 5 and 2 decimal places.

- [[#CO46-7][[[file:callouts/7.png]]]]  :: Unicode character name.

Running [[file:ch04.html#ex_numerics_demo][Example 4-21]] gives you the result in [[file:ch04.html#numerics_demo_fig][Figure 4-3]].



[[file:images/flup_0403.png]]

Figure 4-3. Nine numeric characters and metadata about them; re_dig means the character matches the regular expression r'd';

The sixth column of [[file:ch04.html#numerics_demo_fig][Figure 4-3]] is the result of calling =unicodedata.numeric(char)= on the character. It shows that Unicode knows the numeric value of symbols that represent numbers. So if you want to create a spreadsheet application that supports Tamil digits or Roman numerals, go for it!

[[file:ch04.html#numerics_demo_fig][Figure 4-3]] shows that the regular expression =r'd'= matches the digit “1” and the Devanagari digit 3, but not some other characters that are considered digits by the =isdigit= function. The =re= module is not as savvy about Unicode as it could be. The new =regex= module available in PyPI was designed to eventually replace =re= and provides better Unicode support.^{[[[#ftn.id469746][30]]]} We'll come back to the =re= module in the next section.

Throughout this chapter we've used several =unicodedata= functions, but there are many more we did not cover. See the standard library documentation for the [[https://docs.python.org/3/library/unicodedata.html][=unicodedata= module]].

We will wrap up our tour of =str= versus =bytes= with a quick look at a new trend: dual-mode APIs offering functions that accept =str= or =bytes= arguments with special handling depending on the type.

** Dual-Mode str and bytes APIs


The standard library has functions that accept =str= or =bytes= arguments and behave differently depending on the type. Some examples are in the =re= and =os= modules.

*** str Versus bytes in Regular Expressions
    :PROPERTIES:
    :CUSTOM_ID: _str_versus_bytes_in_regular_expressions
    :CLASS: title
    :END:

If you build a regular expression with =bytes=, patterns such as =d= and =w= only match ASCII characters; in contrast, if these patterns are given as =str=, they match Unicode digits or letters beyond ASCII. [[file:ch04.html#ex_re_demo][Example 4-22]] and [[file:ch04.html#fig_re_demo][Figure 4-4]] compare how letters, ASCII digits, superscripts, and Tamil digits are matched by =str= and =bytes= patterns.



Example 4-22. ramanujan.py: compare behavior of simple str and bytes regular expressions

#+BEGIN_EXAMPLE
    import re

    re_numbers_str = re.compile(r'd+')      
    re_words_str = re.compile(r'w+')
    re_numbers_bytes = re.compile(rb'd+')   
    re_words_bytes = re.compile(rb'w+')

    text_str = ("Ramanujan saw u0be7u0bedu0be8u0bef"   
                " as 1729 = 1³ + 12³ = 9³ + 10³.")         

    text_bytes = text_str.encode('utf_8')   

    print('Text', repr(text_str), sep='n  ')
    print('Numbers')
    print('  str  :', re_numbers_str.findall(text_str))       
    print('  bytes:', re_numbers_bytes.findall(text_bytes))   
    print('Words')
    print('  str  :', re_words_str.findall(text_str))         
    print('  bytes:', re_words_bytes.findall(text_bytes))     
#+END_EXAMPLE

- [[#CO47-1][[[file:callouts/1.png]]]]  :: The first two regular expressions are of the =str= type.

- [[#CO47-2][[[file:callouts/2.png]]]]  :: The last two are of the =bytes= type.

- [[#CO47-3][[[file:callouts/3.png]]]]  :: Unicode text to search, containing the Tamil digits for 1729 (the logical line continues until the right parenthesis token).

- [[#CO47-4][[[file:callouts/4.png]]]]  :: This string is joined to the previous one at compile time (see [[http://bit.ly/1IqE2vH][“2.4.2. String literal concatenation”]] in /The Python Language Reference/).

- [[#CO47-5][[[file:callouts/5.png]]]]  :: A =bytes= string is needed to search with the =bytes= regular expressions.

- [[#CO47-6][[[file:callouts/6.png]]]]  :: The =str= pattern =r'd+'= matches the Tamil and ASCII digits.

- [[#CO47-7][[[file:callouts/7.png]]]]  :: The =bytes= pattern =rb'd+'= matches only the ASCII bytes for digits.

- [[#CO47-8][[[file:callouts/8.png]]]]  :: The =str= pattern =r'w+'= matches the letters, superscripts, Tamil, and ASCII digits.

- [[#CO47-9][[[file:callouts/9.png]]]]  :: The =bytes= pattern =rb'w+'= matches only the ASCII bytes for letters and digits.



[[file:images/flup_0404.png]]

Figure 4-4. Screenshot of running ramanujan.py from [[file:ch04.html#ex_re_demo][Example 4-22]]

[[file:ch04.html#ex_re_demo][Example 4-22]] is a trivial example to make one point: you can use regular expressions on =str= and =bytes=, but in the second case bytes outside of the ASCII range are treated as nondigits and nonword characters.

For =str= regular expressions, there is a =re.ASCII= flag that makes =w=, =W=, =b=, =B=, =d=, =D=, =s=, and =S= perform ASCII-only matching. See the [[https://docs.python.org/3/library/re.html][documentation of the =re= module]] for full details.

Another important dual-mode module is =os=.

*** str Versus bytes on os Functions
    :PROPERTIES:
    :CUSTOM_ID: _str_versus_bytes_on_os_functions
    :CLASS: title
    :END:

The GNU/Linux kernel is not Unicode savvy, so in the real world you may find filenames made of byte sequences that are not valid in any sensible encoding scheme, and cannot be decoded to =str=. File servers with clients using a variety of OSes are particularly prone to this problem.

In order to work around this issue, all =os= module functions that accept filenames or pathnames take arguments as =str= or =bytes=. If one such function is called with a =str= argument, the argument will be automatically converted using the codec named by =sys.getfilesystemencoding()=, and the OS response will be decoded with the same codec. This is almost always what you want, in keeping with the Unicode sandwich best practice.

But if you must deal with (and perhaps fix) filenames that cannot be handled in that way, you can pass =bytes= arguments to the =os= functions to get =bytes= return values. This feature lets you deal with any file or pathname, no matter how many gremlins you may find. See [[file:ch04.html#ex_listdir1][Example 4-23]].



Example 4-23. listdir with str and bytes arguments and results

#+BEGIN_EXAMPLE
    >>> os.listdir('.')  # 
    ['abc.txt', 'digits-of-π.txt']
    >>> os.listdir(b'.')  # 
    [b'abc.txt', b'digits-of-xcfx80.txt']
#+END_EXAMPLE

- [[#CO48-1][[[file:callouts/1.png]]]]  :: The second filename is “digits-of-π.txt” (with the Greek letter pi).

- [[#CO48-2][[[file:callouts/2.png]]]]  :: Given a =byte= argument, =listdir= returns filenames as bytes: =b'xcfx80'= is the UTF-8 encoding of the Greek letter pi).

To help with manual handling of =str= or =bytes= sequences that are file or pathnames, the =os= module provides special encoding and decoding functions:

-  =fsencode(filename)=  :: Encodes =filename= (can be =str= or =bytes=) to =bytes= using the codec named by =sys.getfilesystemencoding()= if =filename= is of type =str=, otherwise returns the =filename= =bytes= unchanged.
-  =fsdecode(filename)=  :: Decodes =filename= (can be =str= or =bytes=) to =str= using the codec named by =sys.getfilesystemencoding()= if =filename= is of type =bytes=, otherwise returns the =filename= =str= unchanged.

On Unix-derived platforms, these functions use the =surrogateescape= error handler (see the sidebar that follows) to avoid choking on unexpected bytes. On Windows, the =strict= error handler is used.

Using surrogateescape to Deal with Gremlins

A trick to deal with unexpected bytes or unknown encodings is the =surrogateescape= codec error handler described in [[https://www.python.org/dev/peps/pep-0383/][PEP 383 --- Non-decodable Bytes in System Character Interfaces]] introduced in Python 3.1.

The idea of this error handler is to replace each nondecodable byte with a code point in the Unicode range from U+DC00 to U+DCFF that lies in the so-called “Low Surrogate Area” of the standard---a code space with no characters assigned, reserved for internal use in applications. On encoding, such code points are converted back to the byte values they replaced. See [[file:ch04.html#ex_listdir][Example 4-24]].



Example 4-24. Using surrogatescape error handling

#+BEGIN_EXAMPLE
    >>> os.listdir('.')  
    ['abc.txt', 'digits-of-π.txt']
    >>> os.listdir(b'.')  
    [b'abc.txt', b'digits-of-xcfx80.txt']
    >>> pi_name_bytes = os.listdir(b'.')[1]  
    >>> pi_name_str = pi_name_bytes.decode('ascii', 'surrogateescape')  
    >>> pi_name_str  
    'digits-of-udccfudc80.txt'
    >>> pi_name_str.encode('ascii', 'surrogateescape')  
    b'digits-of-xcfx80.txt'
#+END_EXAMPLE

- [[#CO49-1][[[file:callouts/1.png]]]]  :: List directory with a non-ASCII filename.

- [[#CO49-2][[[file:callouts/2.png]]]]  :: Let's pretend we don't know the encoding and get filenames as =bytes=.

- [[#CO49-3][[[file:callouts/3.png]]]]  :: =pi_names_bytes= is the filename with the pi character.

- [[#CO49-4][[[file:callouts/4.png]]]]  :: Decode it to =str= using the ='ascii'= codec with ='surrogateescape'=.

- [[#CO49-5][[[file:callouts/5.png]]]]  :: Each non-ASCII byte is replaced by a surrogate code point: ='xcfx80'= becomes ='udccfudc80'=.

- [[#CO49-6][[[file:callouts/6.png]]]]  :: Encode back to ASCII bytes: each surrogate code point is replaced by the byte it replaced.

This ends our exploration of =str= and =bytes=. If you are still with me, congratulations!

** Chapter Summary


We started the chapter by dismissing the notion that =1 character == 1 byte=. As the world adopts Unicode (80% of websites already use UTF-8), we need to keep the concept of text strings separated from the binary sequences that represent them in files, and Python 3 enforces this separation.

After a brief overview of the binary sequence data types---=bytes=, =bytearray=, and =memoryview=---we jumped into encoding and decoding, with a sampling of important codecs, followed by approaches to prevent or deal with the infamous =UnicodeEncodeError=, =UnicodeDecodeError=, and the =SyntaxError= caused by wrong encoding in Python source files.

While on the subject of source code, I presented my position on the debate about non-ASCII identifiers: if the maintainers of the code base want to use a human language that has non-ASCII characters, the identifiers should follow suit---unless the code needs to run on Python 2 as well. But if the project aims to attract an international contributor base, identifiers should be made from English words, and then ASCII suffices.

We then considered the theory and practice of encoding detection in the absence of metadata: in theory, it can't be done, but in practice the Chardet package pulls it off pretty well for a number of popular encodings. Byte order marks were then presented as the only encoding hint commonly found in UTF-16 and UTF-32 files---sometimes in UTF-8 files as well.

In the next section, we demonstrated opening text files, an easy task except for one pitfall: the =encoding== keyword argument is not mandatory when you open a text file, but it should be. If you fail to specify the encoding, you end up with a program that manages to generate “plain text” that is incompatible across platforms, due to conflicting default encodings. We then exposed the different encoding settings that Python uses as defaults and how to detect them: =locale.getpreferredencoding()=, =sys.getfilesystemencoding()=, =sys.getdefaultencoding()=, and the encodings for the standard I/O files (e.g., =sys.stdout.encoding=). A sad realization for Windows users is that these settings often have distinct values within the same machine, and the values are mutually incompatible; GNU/Linux and OSX users, in contrast, live in a happier place where =UTF-8= is the default pretty much everywhere.

Text comparisons are surprisingly complicated because Unicode provides multiple ways of representing some characters, so normalizing is a prerequisite to text matching. In addition to explaining normalization and case folding, we presented some utility functions that you may adapt to your needs, including drastic transformations like removing all accents. We then saw how to sort Unicode text correctly by leveraging the standard =locale= module---with some caveats---and an alternative that does not depend on tricky locale configurations: the external PyUCA package.

Finally, we glanced at the Unicode database (a source of metadata about every character), and wrapped up with brief discussion of dual-mode APIs (e.g., the =re= and =os= modules, where some functions can be called with =str= or =bytes= arguments, prompting different yet fitting results).

** Further Reading


Ned Batchelder's 2012 PyCon US talk [[http://nedbatchelder.com/text/unipain.html][“Pragmatic Unicode --- or --- How Do I Stop the Pain?”]] was outstanding. Ned is so professional that he provides a full transcript of the talk along with the slides and video. Esther Nam and Travis Fischer gave an excellent PyCon 2014 talk “Character encoding and Unicode in Python: How to (╯°□°)╯︵ ┻━┻ with dignity” ([[http://bit.ly/1JzF1MY][slides]], [[http://bit.ly/1JzF37P][video]]), from which I quoted this chapter's short and sweet epigraph: “Humans use text. Computers speak bytes.” Lennart Regebro---one of this book's technical reviewers---presents his “Useful Mental Model of Unicode (UMMU)” in the short post [[https://regebro.wordpress.com/2011/03/23/unconfusing-unicode-what-is-unicode/][“Unconfusing Unicode: What Is Unicode?”]]. Unicode is a complex standard, so Lennart's UMMU is a really useful starting point.

The official [[https://docs.python.org/3/howto/unicode.html][Unicode HOWTO]] in the Python docs approaches the subject from several different angles, from a good historic intro to syntax details, codecs, regular expressions, filenames, and best practices for Unicode-aware I/O (i.e., the Unicode sandwich), with plenty of additional reference links from each section. [[http://www.diveintopython3.net/strings.html][Chapter 4, “Strings”]], of Mark Pilgrim's awesome book [[http://www.diveintopython3.net][/Dive into Python 3/]] also provides a very good intro to Unicode support in Python 3. In the same book, [[http://bit.ly/1IqJ63d][Chapter 15]] describes how the Chardet library was ported from Python 2 to Python 3, a valuable case study given that the switch from the old =str= to the new =bytes= is the cause of most migration pains, and that is a central concern in a library designed to detect encodings.

If you know Python 2 but are new to Python 3, Guido van Rossum's [[http://bit.ly/1IqJ8YH][What's New in Python 3.0]] has 15 bullet points that summarize what changed, with lots of links. Guido starts with the blunt statement: “Everything you thought you knew about binary data and Unicode has changed.” Armin Ronacher's blog post [[http://bit.ly/1IqJcrD][“The Updated Guide to Unicode on Python”]] is deep and highlights some of the pitfalls of Unicode in Python 3 (Armin is not a big fan of Python 3).

Chapter 2, “Strings and Text,” of the /[[http://shop.oreilly.com/product/0636920027072.do][Python Cookbook, Third Edition]]/ (O'Reilly), by David Beazley and Brian K. Jones, has several recipes dealing with Unicode normalization, sanitizing text, and performing text-oriented operations on byte sequences. Chapter 5 covers files and I/O, and it includes “Recipe 5.17. Writing Bytes to a Text File,” showing that underlying any text file there is always a binary stream that may be accessed directly when needed. Later in the cookbook, the =struct= module is put to use in “Recipe 6.11. Reading and Writing Binary Arrays of Structures.”

Nick Coghlan's Python Notes blog has two posts very relevant to this chapter: [[http://bit.ly/1dYuNJa][“Python 3 and ASCII Compatible Binary Protocols”]] and [[http://bit.ly/1dYuRbS][“Processing Text Files in Python 3”]]. Highly recommended.

Binary sequences are about to gain new constructors and methods in Python 3.5, with one of the current constructor signatures being deprecated (see [[https://www.python.org/dev/peps/pep-0467/][PEP 467 --- Minor API improvements for binary sequences]]). Python 3.5 should also see the implementation of [[https://www.python.org/dev/peps/pep-0461/][PEP 461 --- Adding % formatting to bytes and bytearray]].

A list of encodings supported by Python is available at [[https://docs.python.org/3/library/codecs.html#standard-encodings][Standard Encodings]] in the =codecs= module documentation. If you need to get that list programmatically, see how it's done in the [[http://bit.ly/1IqKrqD][//Tools/unicode/listcodecs.py/]] script that comes with the CPython source code.

Martijn Faassen's [[http://bit.ly/1IqKu5I][“Changing the Python Default Encoding Considered Harmful”]] and Tarek Ziadé's [[http://blog.ziade.org/2008/01/08/syssetdefaultencoding-is-evil/][“sys.setdefaultencoding Is Evil”]] explain why the default encoding you get from =sys.getdefaultencoding()= should never be changed, even if you discover how.

The books /[[http://shop.oreilly.com/product/9780596101213.do][Unicode Explained]]/ by Jukka K. Korpela (O'Reilly) and [[http://bit.ly/1dYveDl][/Unicode Demystified/]] by Richard Gillam (Addison-Wesley) are not Python-specific but were very helpful as I studied Unicode concepts. [[http://unicodebook.readthedocs.org/index.html][/Programming with Unicode/]] by Victor Stinner is a free, self-published book (Creative Commons BY-SA) covering Unicode in general as well as tools and APIs in the context of the main operating systems and a few programming languages, including Python.

The W3C pages [[http://www.w3.org/International/wiki/Case_folding][Case Folding: An Introduction]] and [[http://www.w3.org/TR/charmod-norm/][Character Model for the World Wide Web: String Matching and Searching]] cover normalization concepts, with the former being a gentle introduction and the latter a working draft written in dry standard-speak---the same tone of the [[http://unicode.org/reports/tr15/][Unicode Standard Annex #15 --- Unicode Normalization Forms]]. The [[http://www.unicode.org/faq/normalization.html][Frequently Asked Questions / Normalization]] from [[http://www.unicode.org/][Unicode.org]] is more readable, as is the [[http://www.macchiato.com/unicode/nfc-faq][NFC FAQ]] by Mark Davis---author of several Unicode algorithms and president of the Unicode Consortium at the time of this writing.

Soapbox

*What Is “Plain Text”?*

For anyone who deals with non-English text on a daily basis, “plain text” does not imply “ASCII.” The [[http://www.unicode.org/glossary/#plain_text][Unicode Glossary]] defines /plain text/ like this:

#+BEGIN_QUOTE
  Computer-encoded text that consists only of a sequence of code points from a given standard, with no other formatting or structural information.
#+END_QUOTE

That definition starts very well, but I don't agree with the part after the comma. HTML is a great example of a plain-text format that carries formatting and structural information. But it's still plain text because every byte in such a file is there to represent a text character, usually using UTF-8. There are no bytes with nontext meaning, as you can find in a /.png/ or /.xls/ document where most bytes represent packed binary values like RGB values and floating-point numbers. In plain text, numbers are represented as sequences of digit characters.

I am writing this book in a plain-text format called---ironically---[[http://www.methods.co.nz/asciidoc/][AsciiDoc]], which is part of the toolchain of O'Reilly's excellent [[https://atlas.oreilly.com/][Atlas book publishing platform]]. AsciiDoc source files are plain text, but they are UTF-8, not ASCII. Otherwise, writing this chapter would have been really painful. Despite the name, AsciiDoc is just great.

The world of Unicode is constantly expanding and, at the edges, tool support is not always there. That's why I had to use images for Figures [[file:ch04.html#encodings_demo_fig][4-1]], [[file:ch04.html#numerics_demo_fig][4-3]], and [[file:ch04.html#fig_re_demo][4-4]]: not all characters I wanted to show were available in the fonts used to render the book. On the other hand, the Ubuntu 14.04 and OSX 10.9 terminals display them perfectly well---including the Japanese characters for the word “mojibake”: 文字化け.

*Unicode Riddles*

Imprecise qualifiers such as “often,” “most,” and “usually” seem to pop up whenever I write about Unicode normalization. I regret the lack of more definitive advice, but there are so many exceptions to the rules in Unicode that it is hard to be absolutely positive.

For example, the µ (micro sign) is considered a “compatibility character” but the Ω (ohm) and Å (Ångström) symbols are not. The difference has practical consequences: NFC normalization---recommended for text matching---replaces the Ω (ohm) by Ω (uppercase Grek omega) and the Å (Ångström) by Å (uppercase A with ring above). But as a “compatibility character” the µ (micro sign) is not replaced by the visually identical μ (lowercase Greek mu), except when the stronger NFKC or NFKD normalizations are applied, and these transformations are lossy.

I understand the µ (micro sign) is in Unicode because it appears in the =latin1= encoding and replacing it with the Greek mu would break round-trip conversion. After all, that's why the micro sign is a “compatibility character.” But if the ohm and Ångström symbols are not in Unicode for compatibility reasons, then why have them at all? There are already code points for the =GREEK CAPITAL LETTER OMEGA= and the =LATIN CAPITAL LETTER A WITH RING ABOVE=, which look the same and replace them on NFC normalization. Go figure.

My take after many hours studying Unicode: it is hugely complex and full of special cases, reflecting the wonderful variety of human languages and the politics of industry standards.

*How Are str Represented in RAM?*

The official Python docs avoid the issue of how the code points of a =str= are stored in memory. This is, after all, an implementation detail. In theory, it doesn't matter: whatever the internal representation, every =str= must be encoded to =bytes= on output.

In memory, Python 3 stores each =str= as a sequence of code points using a fixed number of bytes per code point, to allow efficient direct access to any character or slice.

Before Python 3.3, CPython could be compiled to use either 16 or 32 bits per code point in RAM; the former was a “narrow build,” and the latter a “wide build.” To know which you have, check the value of =sys.maxunicode=: 65535 implies a “narrow build” that can't handle code points above U+FFFF transparently. A “wide build” doesn't have this limitation, but consumes a lot of memory: 4 bytes per character, even while the vast majority of code points for Chinese ideographs fit in 2 bytes. Neither option was great, so you had to choose depending on your needs.

Since Python 3.3, when creating a new =str= object, the interpreter checks the characters in it and chooses the most economic memory layout that is suitable for that particular =str=: if there are only characters in the =latin1= range, that =str= will use just one byte per code point. Otherwise, 2 or 4 bytes per code point may be used, depending on the =str=. This is a simplification; for the full details, look up [[https://www.python.org/dev/peps/pep-0393/][PEP 393 --- Flexible String Representation]].

The flexible string representation is similar to the way the =int= type works in Python 3: if the integer fits in a machine word, it is stored in one machine word. Otherwise, the interpreter switches to a variable-length representation like that of the Python 2 =long= type. It is nice to see the spread of good ideas.



--------------


^{[[[#id714150][19]]]} Slide 12 of PyCon 2014 talk “Character Encoding and Unicode in Python” ([[http://bit.ly/1JzF1MY][slides]], [[http://bit.ly/1JzF37P][video]]).


^{[[[#id810926][20]]]} [[https://pillow.readthedocs.org/en/latest/][Pillow]] is PIL's most active fork.


^{[[[#id817742][21]]]} As of September, 2014, [[http://bit.ly/w3techs-en][W3Techs: Usage of Character Encodings for Websites]] claims that 81.4% of sites use UTF-8, while [[http://trends.builtwith.com/encoding][Built With: Encoding Usage Statistics]] estimates 79.4%.


^{[[[#id868354][22]]]} I first saw the term “Unicode sandwich” in Ned Batchelder's excellent [[http://nedbatchelder.com/text/unipain/unipain.html][“Pragmatic Unicode” talk]] at US PyCon 2012.


^{[[[#id865027][23]]]} Python 2.6 or 2.7 users have to use =io.open()= to get automatic decoding/encoding when reading/writing.


^{[[[#id721384][24]]]} While researching this subject, I did not find a list of situations when Python 3 internally converts =bytes= to =str=. Python core developer Antoine Pitrou says on the [[http://bit.ly/1IqvSU2][=comp.python.devel= list]] that CPython internal functions that depend on such conversions “don't get a lot of use in py3k.”


^{[[[#id636300][25]]]} The Python 2 =sys.setdefaultencoding= function was misused and is no longer documented in Python 3. It was intended for use by the core developers when the internal default encoding of Python was still undecided. In the same [[http://bit.ly/1IqvN2J][=comp.python.devel= thread]], Marc-André Lemburg states that the =sys.setdefaultencoding= must never be called by user code and the only values supported by CPython are ='ascii'= in Python 2 and ='utf-8'= in Python 3.


^{[[[#id564933][26]]]} Curiously, the micro sign is considered a “compatibility character” but the ohm symbol is not. The end result is that NFC doesn't touch the micro sign but changes the ohm symbol to capital omega, while NFKC and NFKD change both the ohm and the micro into other characters.


^{[[[#id908404][27]]]} Diacritics affect sorting only in the rare case when they are the only difference between two words---in that case, the word with a diacritic is sorted after the plain word.


^{[[[#id410129][28]]]} Thanks to Leonardo Rachael who went beyond his duties as tech reviewer and researched these Windows details, even though he is a GNU/Linux user himself.


^{[[[#id980616][29]]]} Again, I could not find a solution, but did find other people reporting the same problem. Alex Martelli, one of the tech reviewers, had no problem using =setlocale= and =locale.strxfrm= on his Mac with OSX 10.9. In summary: your mileage may vary.


^{[[[#id469746][30]]]} Although it was not better than =re= at identifying digits in this particular sample.


d721384][24]]]} While researching this subject, I did not find a list of situations when Python 3 internally converts =bytes= to =str=. Python core developer Antoine Pitrou says on the [[http://bit.ly/1IqvSU2][=comp.python.devel= list]] that CPython internal functions that depend on such conversions “don't get a lot of use in py3k.”


^{[[[#id636300][25]]]} The Python 2 =sys.setdefaultencoding= function was misused and is no longer documented in Python 3. It was intended for use by the core developers when the internal default encoding of Python was still undecided. In the same [[http://bit.ly/1IqvN2J][=comp.python.devel= thread]], Marc-André Lemburg states that the =sys.setdefaultencoding= must never be called by user code and the only values supported by CPython are ='ascii'= in Python 2 and ='utf-8'= in Python 3.


^{[[[#id564933][26]]]} Curiously, the micro sign is considered a “compatibility character” but the ohm symbol is not. The end result is that NFC doesn't touch the micro sign but changes the ohm symbol to capital omega, while NFKC and NFKD change both the ohm and the micro into other characters.


^{[[[#id908404][27]]]} Diacritics affect sorting only in the rare case when they are the only difference between two words---in that case, the word with a diacritic is sorted after the plain word.


^{[[[#id410129][28]]]} Thanks to Leonardo Rachael who went beyond his duties as tech reviewer and researched these Windows details, even though he is a GNU/Linux user himself.


^{[[[#id980616][29]]]} Again, I could not find a solution, but did find other people reporting the same problem. Alex Martelli, one of the tech reviewers, had no problem using =setlocale= and =locale.strxfrm= on his Mac with OSX 10.9. In summary: your mileage may vary.


^{[[[#id469746][30]]]} Although it was not better than =re= at identifying digits in this particular sample.


[30]]]} Although it was not better than =re= at identifying digits in this particular sample.


