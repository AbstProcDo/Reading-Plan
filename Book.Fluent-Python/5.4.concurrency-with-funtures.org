** Chapter 17. Concurrency with Futures


Python has supported threads since its release 0.9.8 (1993); =concurrent.futures= is just the latest way of using them. In Python 3, the original =thread= module was deprecated in favor of the higher-level [[https://docs.python.org/3/library/threading.html][=threading= module]].^{[[[#ftn.id416812][154]]]} If =futures.ThreadPoolExecutor= is not flexible enough for a certain job, you may need to build your own solution out of basic =threading= components such as =Thread=, =Lock=, =Semaphore=, etc.---possibly using the thread-safe queues of the [[https://docs.python.org/3/library/queue.html][=queue= module]] for passing data between threads. Those moving parts are encapsulated by =futures.ThreadPoolExecutor=.

For CPU-bound work, you need to sidestep the GIL by launching multiple processes. The =futures.ProcessPoolExecutor= is the easiest way to do it. But again, if your use case is complex, you'll need more advanced tools. The [[https://docs.python.org/3/library/multiprocessing.html][=multiprocessing= package]] emulates the =threading= API but delegates jobs to multiple processes. For simple programs, =multiprocessing= can replace =threading= with few changes. But =multiprocessing= also offers facilities to solve the biggest challenge faced by collaborating processes: how to pass around data.

** Chapter Summary


We started the chapter by comparing two concurrent HTTP clients with a sequential one, demonstrating significant performance gains over the sequential script.

After studying the first example based on =concurrent.futures=, we took a closer look at future objects, either instances of =concurrent.futures.Future=, or =asyncio.Future=, emphasizing what these classes have in common (their differences will be emphasized in [[file:ch18.html][Chapter 18]]). We saw how to create futures by calling =Executor.submit(…)=, and iterate over completed futures with =concurrent.futures.as_completed(…)=.

Next, we saw why Python threads are well suited for I/O-bound applications, despite the GIL: every standard library I/O function written in C releases the GIL, so while a given thread is waiting for I/O, the Python scheduler can switch to another thread. We then discussed the use of multiple processes with the =concurrent.futures.ProcessPoolExecutor= class, to go around the GIL and use multiple CPU cores to run cryptographic algorithms, achieving speedups of more than 100% when using four workers.

In the following section, we took a close look at how the =concurrent.futures.ThreadPoolExecutor= works, with a didactic example launching tasks that did nothing for a few seconds, except displaying their status with a timestamp.

Next we went back to the flag downloading examples. Enhancing them with a progress bar and proper error handling prompted further exploration of the =future.as_completed= generator function showing a common pattern: storing futures in a =dict= to link further information to them when submitting, so that we can use that information when the future comes out of the =as_completed= iterator.

We concluded the coverage of concurrency with threads and processes with a brief reminder of the lower-level, but more flexible =threading= and =multiprocessing= modules, which represent the traditional way of leveraging threads and processes in Python.

** Further Reading


The =concurrent.futures= package was contributed by Brian Quinlan, who presented it in a great talk titled [[http://bit.ly/1JIuZJy][“The Future Is Soon!”]] at PyCon Australia 2010. Quinlan's talk has no slides; he shows what the library does by typing code directly in the Python console. As a motivating example, the presentation features a short video with XKCD cartoonist/programmer Randall Munroe making an unintended DOS attack on Google Maps to build a colored map of driving times around his city. The formal introduction to the library is [[https://www.python.org/dev/peps/pep-3148/][PEP 3148 - =futures= - execute computations asynchronously]]. In the PEP, Quinlan wrote that the =concurrent.futures= library was “heavily influenced by the Java =java.util.concurrent= package.”

/Parallel Programming with Python/ (Packt), by Jan Palach, covers several tools for concurrent programming, including the =concurrent.futures=, =threading=, and =multiprocessing= modules. It goes beyond the standard library to discuss [[http://bit.ly/1JIv1kA][Celery]], a task queue used to distribute work across threads and processes, even on different machines. In the Django community, Celery is probably the most widely used system to offload heavy tasks such as PDF generation to other processes, thus avoiding delays in producing an HTTP response.

In the Beazley and Jones /[[http://shop.oreilly.com/product/0636920027072.do][Python Cookbook, 3E]]/ (O'Reilly) there are recipes using =concurrent.futures= starting with “Recipe 11.12. Understanding Event-Driven I/O.” “Recipe 12.7. Creating a Thread Pool” shows a simple TCP echo server, and “Recipe 12.8. Performing Simple Parallel Programming” offers a very practical example: analyzing a whole directory of =gzip= compressed Apache logfiles with the help of a =ProcessPoolExecutor=. For more about threads, the entire Chapter 12 of Beazley and Jones is great, with special mention to “Recipe 12.10. Defining an Actor Task,” which demonstrates the Actor model: a proven way of coordinating threads through message passing.

Brett Slatkin's [[http://www.effectivepython.com/][/Effective Python/]] (Addison-Wesley) has a multitopic chapter about concurrency, including coverage of coroutines, =concurrent.futures= with threads and processes, and the use of locks and queues for thread programming without the =ThreadPoolExecutor=.

/[[http://shop.oreilly.com/product/0636920028963.do][High Performance Python]]/ (O'Reilly) by Micha Gorelick and Ian Ozsvald and /The Python Standard Library by Example/ (Addison-Wesley), by Doug Hellmann, also cover threads and processes.

For a modern take on concurrency without threads or callbacks, /Seven Concurrency Models in Seven Weeks/, by Paul Butcher (Pragmatic Bookshelf) is an excellent read. I love its subtitle: “When Threads Unravel.” In that book, threads and locks are covered in Chapter 1, and the remaining six chapters are devoted to modern alternatives to concurrent programming, as supported by different languages. Python, Ruby, and JavaScript are not among them.

If you are intrigued about the GIL, start with the /Python Library and Extension FAQ/ ([[http://bit.ly/1HGtb0F][“Can't we get rid of the Global Interpreter Lock?”]]). Also worth reading are posts by Guido van Rossum and Jesse Noller (contributor of the =multiprocessing= package): [[http://bit.ly/1HGtcBF][“It isn't Easy to Remove the GIL”]] and [[http://bit.ly/1JIvgwd][“Python Threads and the Global Interpreter Lock.”]] Finally, David Beazley has a detailed exploration on the inner workings of the GIL: [[http://www.dabeaz.com/GIL/][“Understanding the Python GIL.”]]^{[[[#ftn.id963851][155]]]} In slide #54 of the [[http://bit.ly/1HGtCrK][presentation]], Beazley reports some alarming results, including a 20× increase in processing time for a particular benchmark with the new GIL algorithm introduced in Python 3.2. However, Beazley apparently used an empty =while True: pass= to simulate CPU-bound work, and that is not realistic. The issue is not significant with real workloads, according to [[http://bugs.python.org/issue7946#msg223110][a comment]] by Antoine Pitrou---who implemented the new GIL algorithm---in the bug report submitted by Beazley.

While the GIL is real problem and is not likely to go away soon, Jesse Noller and Richard Oudkerk contributed a library to make it easier to work around it in CPU-bound applications: the =multiprocessing= package, which emulates the =threading= API across processes, along with supporting infrastructure of locks, queues, pipes, shared memory, etc. The package was introduced in [[https://www.python.org/dev/peps/pep-0371/][PEP 371 --- Addition of the multiprocessing package to the standard library]]. The [[http://bit.ly/multi-docs][official documentation for the package]] is a 93 KB /.rst/ file---that's about 63 pages---making it one of the longest chapters in the Python standard library. Multiprocessing is the basis for the =concurrent.futures.ProcessPoolExecutor=.

For CPU- and data-intensive parallel processing, a new option with a lot of momentum in the big data community is the [[https://spark.apache.org][Apache Spark]] distributed computing engine, offering a friendly Python API and support for Python objects as data, as shown in their [[https://spark.apache.org/examples.html][examples page]].

Two elegant and super easy libraries for parallelizing tasks over processes are [[https://pypi.python.org/pypi/lelo][=lelo=]] by João S. O. Bueno and [[http://bit.ly/1HGtF6Q][=python-parallelize=]] by Nat Pryce. The =lelo= package defines a =@parallel= decorator that you can apply to any function to magically make it unblocking: when you call the decorated function, its execution is started in another process. Nat Pryce's =python-parallelize= package provides a =parallelize= generator that you can use to distribute the execution of a =for= loop over multiple CPUs. Both packages use the =multiprocessing= module under the covers.

Soapbox

*Thread Avoidance*

#+BEGIN_QUOTE
  Concurrency: one of the most difficult topics in computer science (usually best avoided).^{[[[#ftn.id748281][156]]]}

  --- David Beazley /Python coach and mad scientist/

#+END_QUOTE

I agree with the apparently contradictory quotes by David Beazley, above, and Michele Simionato at the start of this chapter. After attending a concurrency course at the university---in which “concurrent programming” was equated to managing threads and locks---I came to the conclusion that I don't want to manage threads and locks myself, any more than I want to manage memory allocation and deallocation. Those jobs are best carried out by the systems programmers who have the know-how, the inclination, and the time to get them right---hopefully.

That's why I think the =concurrent.futures= package is exciting: it treats threads, processes, and queues as infrastructure at your service, not something you have to deal with directly. Of course, it's designed with simple jobs in mind, the so-called [[http://bit.ly/1HGtGaR][“embarrassingly parallel”]] problems. But that's a large slice of the concurrency problems we face when writing applications---as opposed to operating systems or database servers, as Simionato points out in that quote.

For “nonembarrassing” concurrency problems, threads and locks are not the answer either. Threads will never disappear at the OS level, but every programming language I've found exciting in the last several years provides better, higher-level, concurrency abstractions, as the /Seven Concurrency Models/ book demonstrates. Go, Elixir, and Clojure are among them. Erlang---the implementation language of Elixir---is a prime example of a language designed from the ground up with concurrency in mind. It doesn't excite me for a simple reason: I find its syntax ugly. Python spoiled me that way.

José Valim, well-known as a Ruby on Rails core contributor, designed Elixir with a pleasant, modern syntax. Like Lisp and Clojure, Elixir implements syntactic macros. That's a double-edged sword. Syntactic macros enable powerful DSLs, but the proliferation of sublanguages can lead to incompatible codebases and community fragmentation. Lisp drowned in a flood of macros, with each Lisp shop using its own arcane dialect. Standardizing around Common Lisp resulted in a bloated language. I hope José Valim can inspire the Elixir community to avoid a similar outcome.

Like Elixir, Go is a modern language with fresh ideas. But, in some regards, it's a conservative language, compared to Elixir. Go doesn't have macros, and its syntax is simpler than Python's. Go doesn't support inheritance or operator overloading, and it offers fewer opportunities for metaprogramming than Python. These limitations are considered features. They lead to more predictable behavior and performance. That's a big plus in the highly concurrent, mission-critical settings where Go aims to replace C++, Java, and Python.

While Elixir and Go are direct competitors in the high-concurrency space, their design philosophies appeal to different crowds. Both are likely to thrive. But in the history of programming languages, the conservative ones tend to attract more coders. I'd like to become fluent in Go and Elixir.

*About the GIL*

The GIL simplifies the implementation of the CPython interpreter and of extensions written in C, so we can thank the GIL for the vast number of extensions in C available for Python---and that is certainly one of the key reasons why Python is so popular today.

For many years, I was under the impression that the GIL made Python threads nearly useless beyond toy applications. It was not until I discovered that /every/ blocking I/O call in the standard library releases the GIL that I realized Python threads are excellent for I/O-bound systems---the kind of applications customers usually pay me to develop, given my professional experience.

*Concurrency in the Competition*

MRI---the reference implementation of Ruby---also has a GIL, so its threads are under the same limitations as Python's. Meanwhile, JavaScript interpreters don't support user-level threads at all; asynchronous programming with callbacks is their only path to concurrency. I mention this because Ruby and JavaScript are the closest direct competitors to Python as general-purpose, dynamic programming languages.

Looking at the concurrency-savvy new crop of languages, Go and Elixir are probably the ones best positioned to eat Python's lunch. But now we have =asyncio=. If hordes of people believe Node.js with raw callbacks is a viable platform for concurrent programming, how hard can it be to win them over to Python when the =asyncio= ecosystem matures? But that's a topic for the next [[file:ch18.html#ch18-soapbox][Soapbox]].



--------------


^{[[[#id789805][148]]]} From Michele Simionato's post [[http://bit.ly/1JIrYZQ][Threads, processes and concurrency in Python: some thoughts]], subtitled “Removing the hype around the multicore (non) revolution and some (hopefully) sensible comment about threads and other forms of concurrency.”


^{[[[#id507235][149]]]} The images are originally from the [[http://1.usa.gov/1JIsmHJ][CIA World Factbook]], a public-domain, U.S. government publication. I copied them to my site to avoid the risk of launching a DOS attack on CIA.gov.


^{[[[#id440956][150]]]} This is a limitation of the CPython interpreter, not of the Python language itself. Jython and IronPython are not limited in this way; but Pypy, the fastest Python interpreter available, also has a GIL.


^{[[[#id441026][151]]]} Slide 106 of [[http://www.dabeaz.com/finalgenerator/][“Generators: The Final Frontier”]].


^{[[[#id1079796][152]]]} Your mileage may vary: with threads, you never know the exact sequencing of events that should happen practically at the same time; it's possible that, in another machine, you see =loiter(1)= starting before =loiter(0)= finishes, particularly because =sleep= always releases the GIL so Python may switch to another thread even if you sleep for 0s.


^{[[[#id1079267][153]]]} Before configuring Cloudflare, I got HTTP 503 errors---Service Temporarily Unavailable---when testing the scripts with a few dozen concurrent requests on my inexpensive shared host account. Now those errors are gone.


^{[[[#id416812][154]]]} The =threading= module has been available since Python 1.5.1 (1998), yet some insist on using the old =thread= module. In Python 3, it was renamed to =_thread= to highlight the fact that it's just a low-level implementation detail, and shouldn't be used in application code.


^{[[[#id963851][155]]]} Thanks to Lucas Brunialti for sending me a link to this talk.


^{[[[#id748281][156]]]} Slide #9 from [[http://www.dabeaz.com/coroutines/][“A Curious Course on Coroutines and Concurrency,”]] tutorial presented at PyCon 2009.


or/][“Generators: The Final Frontier”]].


^{[[[#id1079796][152]]]} Your mileage may vary: with threads, you never know the exact sequencing of events that should happen practically at the same time; it's possible that, in another machine, you see =loiter(1)= starting before =loiter(0)= finishes, particularly because =sleep= always releases the GIL so Python may switch to another thread even if you sleep for 0s.


^{[[[#id1079267][153]]]} Before configuring Cloudflare, I got HTTP 503 errors---Service Temporarily Unavailable---when testing the scripts with a few dozen concurrent requests on my inexpensive shared host account. Now those errors are gone.


^{[[[#id416812][154]]]} The =threading= module has been available since Python 1.5.1 (1998), yet some insist on using the old =thread= module. In Python 3, it was renamed to =_thread= to highlight the fact that it's just a low-level implementation detail, and shouldn't be used in application code.


^{[[[#id963851][155]]]} Thanks to Lucas Brunialti for sending me a link to this talk.


^{[[[#id748281][156]]]} Slide #9 from [[http://www.dabeaz.com/coroutines/][“A Curious Course on Coroutines and Concurrency,”]] tutorial presented at PyCon 2009.


us Course on Coroutines and Concurrency,”]] tutorial presented at PyCon 2009.


at is transparent to the user. An example of the latter is the =Executor.map= we saw in [[file:ch17.html#flags_threadpool_ex][Example 17-3]]: it returns an iterator in which =__next__= calls the =result= method of each future, so what we get are the results of the futures, and not the futures themselves.

To get a practical look at futures, we can rewrite [[file:ch17.html#flags_threadpool_ex][Example 17-3]] to use the [[http://bit.ly/1JIsEOW][=concurrent.futures.as_completed=]] function, which takes an iterable of futures and returns an iterator that yields futures as they are done.

Using =futures.as_completed= requires changes to the =download_many= function only. The higher-level =executor.map= call is replaced by two =for= loops: one to create and schedule the futures, the other to retrieve their results. While we are at it, we'll add a few =print= calls to display each future before and after it's done. [[file:ch17.html#flags_threadpool_ac_ex][Example 17-4]] shows the code for a new =download_many= function. The code for =download_many= grew from 5 to 17 lines, but now we get to inspect the mysterious futures. The remaining functions are the same as in [[file:ch17.html#flags_threadpool_ex][Example 17-3]].



Example 17-4. flags_threadpool_ac.py: replacing executor.map with executor.submit and futures.as_completed in the download_many function

#+BEGIN_EXAMPLE
    def download_many(cc_list):
        cc_list = cc_list[:5]   
        with futures.ThreadPoolExecutor(max_workers=3) as executor:   
            to_do = []
            for cc in sorted(cc_list):   
                future = executor.submit(download_one, cc)   
                to_do.append(future)   
                msg = 'Scheduled for {}: {}'
                print(msg.format(cc, future))   

            results = []
            for future in futures.as_completed(to_do):   
                res = future.result()   
                msg = '{} result: {!r}'
                print(msg.format(future, res))  
                results.append(res)

        return len(results)
#+END_EXAMPLE

- [[#CO198-1][[[file:callouts/1.png]]]]  :: For this demonstration, use only the top five most populous countries.

- [[#CO198-2][[[file:callouts/2.png]]]]  :: Hardcode =max_workers= to =3= so we can observe pending futures in the output.

- [[#CO198-3][[[file:callouts/3.png]]]]  :: Iterate over country codes alphabetically, to make it clear that results arrive out of order.

- [[#CO198-4][[[file:callouts/4.png]]]]  :: =executor.submit= schedules the callable to be executed, and returns a =future= representing this pending operation.

- [[#CO198-5][[[file:callouts/5.png]]]]  :: Store each =future= so we can later retrieve them with =as_completed=.

- [[#CO198-6][[[file:callouts/6.png]]]]  :: Display a message with the country code and the respective =future=.

- [[#CO198-7][[[file:callouts/7.png]]]]  :: =as_completed= yields futures as they are completed.

- [[#CO198-8][[[file:callouts/8.png]]]]  :: Get the result of this =future=.

- [[#CO198-9][[[file:callouts/9.png]]]]  :: Display the =future= and its result.

Note that the =future.result()= call will never block in this example because the =future= is coming out of =as_completed=. [[file:ch17.html#flags_threadpool_ac_run][Example 17-5]] shows the output of one run of [[file:ch17.html#flags_threadpool_ac_ex][Example 17-4]].



Example 17-5. Output of flags_threadpool_ac.py

#+BEGIN_EXAMPLE
    $ python3 flags_threadpool_ac.py
    Scheduled for BR: <Future at 0x100791518 state=running>  
    Scheduled for CN: <Future at 0x100791710 state=running>
    Scheduled for ID: <Future at 0x100791a90 state=running>
    Scheduled for IN: <Future at 0x101807080 state=pending>  
    Scheduled for US: <Future at 0x101807128 state=pending>
    CN <Future at 0x100791710 state=finished returned str> result: 'CN'  
    BR ID <Future at 0x100791518 state=finished returned str> result: 'BR'  
    <Future at 0x100791a90 state=finished returned str> result: 'ID'
    IN <Future at 0x101807080 state=finished returned str> result: 'IN'
    US <Future at 0x101807128 state=finished returned str> result: 'US'

    5 flags downloaded in 0.70s
#+END_EXAMPLE

- [[#CO199-1][[[file:callouts/1.png]]]]  :: The futures are scheduled in alphabetical order; the =repr()= of a future shows its state: the first three are =running=, because there are three worker threads.

- [[#CO199-2][[[file:callouts/2.png]]]]  :: The last two futures are =pending=, waiting for worker threads.

- [[#CO199-3][[[file:callouts/3.png]]]]  :: The first =CN= here is the output of =download_one= in a worker thread; the rest of the line is the output of =download_many=.

- [[#CO199-4][[[file:callouts/4.png]]]]  :: Here two threads output codes before =download_many= in the main thread can display the result of the first thread.

*** Note
    :PROPERTIES:
    :CUSTOM_ID: note-1
    :CLASS: title
    :END:

If you run /flags_threadpool_ac.py/ several times, you'll see the order of the results varying. Increasing the =max_workers= argument to =5= will increase the variation in the order of the results. Decreasing it to =1= will make this code run sequentially, and the order of the results will always be the order of the =submit= calls.

We saw two variants of the download script using =concurrent.futures=: [[file:ch17.html#flags_threadpool_ex][Example 17-3]] with =ThreadPoolExecutor.map= and [[file:ch17.html#flags_threadpool_ac_ex][Example 17-4]] with =futures.as_completed=. If you are curious about the code for /flags_asyncio.py/, you may peek at [[file:ch18.html#flags_asyncio_ex][Example 18-5]] in [[file:ch18.html][Chapter 18]].

Strictly speaking, none of the concurrent scripts we tested so far can perform downloads in parallel. The =concurrent.futures= examples are limited by the GIL, and the /flags_asyncio.py/ is single-threaded.

At this point, you may have questions about the informal benchmarks we just did:

- How can /flags_threadpool.py/ perform 5× faster than /flags.py/ if Python threads are limited by a Global Interpreter Lock (GIL) that only lets one thread run at any time?
- How can /flags_asyncio.py/ perform 5× faster than /flags.py/ when both are single threaded?

I will answer the second question in [[file:ch18.html#around_blocking_calls_sec][Running Circling Around Blocking Calls]].

Read on to understand why the GIL is nearly harmless with I/O-bound processing.

** Blocking I/O and the GIL


The CPython interpreter is not thread-safe internally, so it has a Global Interpreter Lock (GIL), which allows only one thread at a time to execute Python bytecodes. That's why a single Python process usually cannot use multiple CPU cores at the same time.^{[[[#ftn.id440956][150]]]}

When we write Python code, we have no control over the GIL, but a built-in function or an extension written in C can release the GIL while running time-consuming tasks. In fact, a Python library coded in C can manage the GIL, launch its own OS threads, and take advantage of all available CPU cores. This complicates the code of the library considerably, and most library authors don't do it.

However, all standard library functions that perform blocking I/O release the GIL when waiting for a result from the OS. This means Python programs that are I/O bound can benefit from using threads at the Python level: while one Python thread is waiting for a response from the network, the blocked I/O function releases the GIL so another thread can run.

That's why David Beazley says: “Python threads are great at doing nothing.”^{[[[#ftn.id441026][151]]]}

*** Tip
    :PROPERTIES:
    :CUSTOM_ID: tip-1
    :CLASS: title
    :END:

Every blocking I/O function in the Python standard library releases the GIL, allowing other threads to run. The =time.sleep()= function also releases the GIL. Therefore, Python threads are perfectly usable in I/O-bound applications, despite the GIL.

Now let's take a brief look at a simple way to work around the GIL for CPU-bound jobs using =concurrent.futures=.

** Launching Processes with concurrent.futures


The [[https://docs.python.org/3/library/concurrent.futures.html][=concurrent.futures= documentation page]] is subtitled “Launching parallel tasks”. The package does enable truly parallel computations because it supports distributing work among multiple Python processes using the =ProcessPoolExecutor= class---thus bypassing the GIL and leveraging all available CPU cores, if you need to do CPU-bound processing.

Both =ProcessPoolExecutor= and =ThreadPoolExecutor= implement the generic =Executor= interface, so it's very easy to switch from a thread-based to a process-based solution using =concurrent.futures=.

There is no advantage in using a =ProcessPoolExecutor= for the flags download example or any I/O-bound job. It's easy to verify this; just change these lines in [[file:ch17.html#flags_threadpool_ex][Example 17-3]]:

#+BEGIN_EXAMPLE
    def download_many(cc_list):
        workers = min(MAX_WORKERS, len(cc_list))
        with futures.ThreadPoolExecutor(workers) as executor:
#+END_EXAMPLE

To this:

#+BEGIN_EXAMPLE
    def download_many(cc_list):
        with futures.ProcessPoolExecutor() as executor:
#+END_EXAMPLE

For simple uses, the only notable difference between the two concrete executor classes is that =ThreadPoolExecutor.__init__= requires a =max_workers= argument setting the number of threads in the pool. That is an optional argument in =ProcessPoolExecutor=, and most of the time we don't use it---the default is the number of CPUs returned by =os.cpu_count()=. This makes sense: for CPU-bound processing, it makes no sense to ask for more workers than CPUs. On the other hand, for I/O-bound processing, you may use 10, 100, or 1,000 threads in a =ThreadPoolExecutor=; the best number depends on what you're doing and the available memory, and finding the optimal number will require careful testing.

A few tests revealed that the average time to download the 20 flags increased to 1.8s with a =ProcessPoolExecutor=---compared to 1.4s in the original =ThreadPoolExecutor= version. The main reason for this is likely to be the limit of four concurrent downloads on my four-core machine, against 20 workers in the thread pool version.

The value of =ProcessPoolExecutor= is in CPU-intensive jobs. I did some performance tests with a couple of CPU-bound scripts:

-  arcfour_futures.py  :: Encrypt and decrypt a dozen byte arrays with sizes from 149 KB to 384 KB using a pure-Python implementation of the RC4 algorithm (listing: [[file:apa.html#support_arcfour_futures][Example A-7]]).
-  sha_futures.py  :: Compute the SHA-256 hash of a dozen 1 MB byte arrays with the standard library =hashlib= package, which uses the OpenSSL library (listing: [[file:apa.html#support_sha_futures][Example A-9]]).

Neither of these scripts do I/O except to display summary results. They build and process all their data in memory, so I/O does not interfere with their execution time.

[[file:ch17.html#timings_tbl][Table 17-1]] shows the average timings I got after 64 runs of the RC4 example and 48 runs of the SHA example. The timings include the time to actually spawn the worker processes.



Table 17-1. Time and speedup factor for the RC4 and SHA examples with one to four workers on an Intel Core i7 2.7 GHz quad-core machine, using Python 3.4

Workers

RC4 time

RC4 factor

SHA time

SHA factor

1

11.48s

1.00x

22.66s

1.00x

2

8.65s

1.33x

14.90s

1.52x

3

6.04s

1.90x

11.91s

1.90x

4

5.58s

2.06x

10.89s

2.08x

In summary, for cryptographic algorithms, you can expect to double the performance by spawning four worker processes with a =ProcessPoolExecutor=, if you have four CPU cores.

For the pure-Python RC4 example, you can get results 3.8 times faster if you use PyPy and four workers, compared with CPython and four workers. That's a speedup of 7.8 times in relation to the baseline of one worker with CPython in [[file:ch17.html#timings_tbl][Table 17-1]].

*** Tip
    :PROPERTIES:
    :CUSTOM_ID: tip-2
    :CLASS: title
    :END:

If you are doing CPU-intensive work in Python, you should try [[http://pypy.org/][PyPy]]. The /arcfour_futures.py/ example ran from 3.8 to 5.1 times faster using PyPy, depending on the number of workers used. I tested with PyPy 2.4.0, which is compatible with Python 3.2.5, so it has =concurrent.futures= in the standard library.

Now let's investigate the behavior of a thread pool with a demonstration program that launches a pool with three workers, running five callables that output timestamped messages.

** Experimenting with Executor.map


The simplest way to run several callables concurrently is with the =Executor.map= function we first saw in [[file:ch17.html#flags_threadpool_ex][Example 17-3]]. [[file:ch17.html#demo_executor_map_ex][Example 17-6]] is a script to demonstrate how =Executor.map= works in some detail. Its output appears in [[file:ch17.html#demo_executor_map_run][Example 17-7]].



Example 17-6. demo_executor_map.py: Simple demonstration of the map method of ThreadPoolExecutor

#+BEGIN_EXAMPLE
    from time import sleep, strftime
    from concurrent import futures


    def display(*args):   
        print(strftime('[%H:%M:%S]'), end=' ')
        print(*args)


    def loiter(n):   
        msg = '{}loiter({}): doing nothing for {}s...'
        display(msg.format('t'*n, n, n))
        sleep(n)
        msg = '{}loiter({}): done.'
        display(msg.format('t'*n, n))
        return n * 10   


    def main():
        display('Script starting.')
        executor = futures.ThreadPoolExecutor(max_workers=3)   
        results = executor.map(loiter, range(5))   
        display('results:', results)  # .
        display('Waiting for individual results:')
        for i, result in enumerate(results):   
            display('result {}: {}'.format(i, result))


    main()
#+END_EXAMPLE

- [[#CO200-1][[[file:callouts/1.png]]]]  :: This function simply prints whatever arguments it gets, preceded by a timestamp in the format =[HH:MM:SS]=.

- [[#CO200-2][[[file:callouts/2.png]]]]  :: =loiter= does nothing except display a message when it starts, sleep for /n/ seconds, then display a message when it ends; tabs are used to indent the messages according to the value of /n/.

- [[#CO200-3][[[file:callouts/3.png]]]]  :: =loiter= returns =n * 10= so we can see how to collect results.

- [[#CO200-4][[[file:callouts/4.png]]]]  :: Create a =ThreadPoolExecutor= with three threads.

- [[#CO200-5][[[file:callouts/5.png]]]]  :: Submit five tasks to the =executor= (because there are only three threads, only three of those tasks will start immediately: the calls =loiter(0)=, =loiter(1)=, and =loiter(2)=); this is a nonblocking call.

- [[#CO200-6][[[file:callouts/6.png]]]]  :: Immediately display the =results= of invoking =executor.map=: it's a generator, as the output in [[file:ch17.html#demo_executor_map_run][Example 17-7]] shows.

- [[#CO200-7][[[file:callouts/7.png]]]]  :: The =enumerate= call in the =for= loop will implicitly invoke =next(results)=, which in turn will invoke =_f.result()= on the (internal) =_f= future representing the first call, =loiter(0)=. The =result= method will block until the future is done, therefore each iteration in this loop will have to wait for the next result to be ready.

I encourage you to run [[file:ch17.html#demo_executor_map_ex][Example 17-6]] and see the display being updated incrementally. While you're at it, play with the =max_workers= argument for the =ThreadPoolExecutor= and with the =range= function that produces the arguments for the =executor.map= call---or replace it with lists of handpicked values to create different delays.

[[file:ch17.html#demo_executor_map_run][Example 17-7]] shows a sample run of [[file:ch17.html#demo_executor_map_ex][Example 17-6]].



Example 17-7. Sample run of demo_executor_map.py from [[file:ch17.html#demo_executor_map_ex][Example 17-6]]

#+BEGIN_EXAMPLE
    $ python3 demo_executor_map.py
    [15:56:50] Script starting.  
    [15:56:50] loiter(0): doing nothing for 0s...  
    [15:56:50] loiter(0): done.
    [15:56:50]      loiter(1): doing nothing for 1s...  
    [15:56:50]              loiter(2): doing nothing for 2s...
    [15:56:50] results: <generator object result_iterator at 0x106517168>  
    [15:56:50]                      loiter(3): doing nothing for 3s...  
    [15:56:50] Waiting for individual results:
    [15:56:50] result 0: 0  
    [15:56:51]      loiter(1): done. 
    [15:56:51]                              loiter(4): doing nothing for 4s...
    [15:56:51] result 1: 10  
    [15:56:52]              loiter(2): done.  
    [15:56:52] result 2: 20
    [15:56:53]                      loiter(3): done.
    [15:56:53] result 3: 30
    [15:56:55]                              loiter(4): done.  
    [15:56:55] result 4: 40
#+END_EXAMPLE

- [[#CO201-1][[[file:callouts/1.png]]]]  :: This run started at 15:56:50.

- [[#CO201-2][[[file:callouts/2.png]]]]  :: The first thread executes =loiter(0)=, so it will sleep for 0s and return even before the second thread has a chance to start, but YMMV.^{[[[#ftn.id1079796][152]]]}

- [[#CO201-3][[[file:callouts/3.png]]]]  :: =loiter(1)= and =loiter(2)= start immediately (because the thread pool has three workers, it can run three functions concurrently).

- [[#CO201-4][[[file:callouts/4.png]]]]  :: This shows that the =results= returned by =executor.map= is a generator; nothing so far would block, regardless of the number of tasks and the =max_workers= setting.

- [[#CO201-5][[[file:callouts/5.png]]]]  :: Because =loiter(0)= is done, the first worker is now available to start the fourth thread for =loiter(3)=.

- [[#CO201-6][[[file:callouts/6.png]]]]  :: This is where execution may block, depending on the parameters given to the =loiter= calls: the =__next__= method of the =results= generator must wait until the first future is complete. In this case, it won't block because the call to =loiter(0)= finished before this loop started. Note that everything up to this point happened within the same second: 15:56:50.

- [[#CO201-7][[[file:callouts/7.png]]]]  :: =loiter(1)= is done one second later, at 15:56:51. The thread is freed to start =loiter(4)=.

- [[#CO201-8][[[file:callouts/8.png]]]]  :: The result of =loiter(1)= is shown: 10. Now the =for= loop will block waiting for the result of =loiter(2)=.

- [[#CO201-9][[[file:callouts/9.png]]]]  :: The pattern repeats: =loiter(2)= is done, its result is shown; same with =loiter(3)=.

- [[#CO201-10][[[file:callouts/10.png]]]]  :: There is a 2s delay until =loiter(4)= is done, because it started at 15:56:51 and did nothing for 4s.

The =Executor.map= function is easy to use but it has a feature that may or may not be helpful, depending on your needs: it returns the results exactly in the same order as the calls are started: if the first call takes 10s to produce a result, and the others take 1s each, your code will block for 10s as it tries to retrieve the first result of the generator returned by =map=. After that, you'll get the remaining results without blocking because they will be done. That's OK when you must have all the results before proceeding, but often it's preferable to get the results as they are ready, regardless of the order they were submitted. To do that, you need a combination of the =Executor.submit= method and the =futures.as_completed= function, as we saw in [[file:ch17.html#flags_threadpool_ac_ex][Example 17-4]]. We'll come back to this technique in [[file:ch17.html#using_futures_as_completed_sec][Using futures.as_completed]].

*** Tip
    :PROPERTIES:
    :CUSTOM_ID: tip-3
    :CLASS: title
    :END:

The combination of =executor.submit= and =futures.as_completed= is more flexible than =executor.map= because you can =submit= different callables and arguments, while =executor.map= is designed to run the same callable on the different arguments. In addition, the set of futures you pass to =futures.as_completed= may come from more than one executor---perhaps some were created by a =ThreadPoolExecutor= instance while others are from a =ProcessPoolExecutor=.

In the next section, we will resume the flag download examples with new requirements that will force us to iterate over the results of =futures.as_completed= instead of using =executor.map=.

** Downloads with Progress Display and Error Handling


As mentioned, the scripts in [[file:ch17.html#ex_web_downloads_sec][Example: Web Downloads in Three Styles]] have no error handling to make them easier to read and to contrast the structure of the three approaches: sequential, threaded, and asynchronous.

In order to test the handling of a variety of error conditions, I created the =flags2= examples:

-  flags2_common.py  :: This module contains common functions and settings used by all =flags2= examples, including a =main= function, which takes care of command-line parsing, timing, and reporting results. This is really support code, not directly relevant to the subject of this chapter, so the source code is in [[file:apa.html][Appendix A]], [[file:apa.html#support_flag_utils][Example A-10]].
-  flags2_sequential.py  :: A sequential HTTP client with proper error handling and progress bar display. Its =download_one= function is also used by =flags2_threadpool.py=.
-  flags2_threadpool.py  :: Concurrent HTTP client based on =futures.ThreadPoolExecutor= to demonstrate error handling and integration of the progress bar.
-  flags2_asyncio.py  :: Same functionality as previous example but implemented with =asyncio= and =aiohttp=. This will be covered in [[file:ch18.html#flags2_asyncio_sec][Enhancing the asyncio downloader Script]], in [[file:ch18.html][Chapter 18]].

*** Be Careful When Testing Concurrent Clients
    :PROPERTIES:
    :CUSTOM_ID: be-careful-when-testing-concurrent-clients
    :CLASS: title
    :END:

When testing concurrent HTTP clients on public HTTP servers, you may generate many requests per second, and that's how denial-of-service (DoS) attacks are made. We don't want to attack anyone, just learn how to build high-performance clients. Carefully throttle your clients when hitting public servers. For high-concurrency experiments, set up a local HTTP server for testing. Instructions for doing it are in the [[http://bit.ly/1JIsg2L][/README.rst/]] file in the /17-futures/countries// directory of the [[http://bit.ly/1JItSti][/Fluent Python/ code repository]].

The most visible feature of the =flags2= examples is that they have an animated, text-mode progress bar implemented with the [[https://github.com/noamraph/tqdm][TQDM package]]. I posted a [[https://www.youtube.com/watch?v=M8Z65tAl5l4][108s video on YouTube]] to show the progress bar and contrast the speed of the three =flags2= scripts. In the video, I start with the sequential download, but I interrupt it after 32s because it was going to take more than 5 minutes to hit on 676 URLs and get 194 flags; I then run the threaded and =asyncio= scripts three times each, and every time they complete the job in 6s or less (i.e., more than 60 times faster). [[file:ch17.html#flags2_progress_fig][Figure 17-1]] shows two screenshots: during and after running /flags2_threadpool.py/.



[[file:images/flup_1701.png]]

Figure 17-1. Top-left: flags2_threadpool.py running with live progress bar generated by tqdm; bottom-right: same terminal window after the script is finished.

TQDM is very easy to use, the simplest example appears in an animated /.gif/ in the project's [[https://github.com/noamraph/tqdm/blob/master/README.md][/README.md/]]. If you type the following code in the Python console after installing the =tqdm= package, you'll see an animated progress bar were the comment is:

#+BEGIN_EXAMPLE
    >>> import time
    >>> from tqdm import tqdm
    >>> for i in tqdm(range(1000)):
    ...     time.sleep(.01)
    ...
    >>> # -> progress bar will appear here <-
#+END_EXAMPLE

Besides the neat effect, the =tqdm= function is also interesting conceptually: it consumes any iterable and produces an iterator which, while it's consumed, displays the progress bar and estimates the remaining time to complete all iterations. To compute that estimate, =tqdm= needs to get an iterable that has a =len=, or receive as a second argument the expected number of items. Integrating TQDM with our =flags2= examples provide an opportunity to look deeper into how the concurrent scripts actually work, by forcing us to use the [[http://bit.ly/1JIsEOW][=futures.as_completed=]] and the [[http://bit.ly/1JIufV1][=asyncio.as_completed=]] functions so that =tqdm= can display progress as each future is completed.

The other feature of the =flags2= example is a command-line interface. All three scripts accept the same options, and you can see them by running any of the scripts with the =-h= option. [[file:ch17.html#flags2_help_demo][Example 17-8]] shows the help text.



Example 17-8. Help screen for the scripts in the flags2 series

#+BEGIN_EXAMPLE
    $ python3 flags2_threadpool.py -h
    usage: flags2_threadpool.py [-h] [-a] [-e] [-l N] [-m CONCURRENT] [-s LABEL]
                                [-v]
                                [CC [CC ...]]

    Download flags for country codes. Default: top 20 countries by population.

    positional arguments:
      CC                    country code or 1st letter (eg. B for BA...BZ)

    optional arguments:
      -h, --help            show this help message and exit
      -a, --all             get all available flags (AD to ZW)
      -e, --every           get flags for every possible code (AA...ZZ)
      -l N, --limit N       limit to N first codes
      -m CONCURRENT, --max_req CONCURRENT
                            maximum concurrent requests (default=30)
      -s LABEL, --server LABEL
                            Server to hit; one of DELAY, ERROR, LOCAL, REMOTE
                            (default=LOCAL)
      -v, --verbose         output detailed progress info
#+END_EXAMPLE

All arguments are optional. The most important arguments are discussed next.

One option you can't ignore is =-s/--server=: it lets you choose which HTTP server and base URL will be used in the test. You can pass one of four strings to determine where the script will look for the flags (the strings are case insensitive):

-  =LOCAL=  :: Use =http://localhost:8001/flags=; this is the default. You should configure a local HTTP server to answer at port 8001. I used Nginx for my tests. The [[http://bit.ly/1JIsg2L][/README.rst/]] file for this chapter's example code explains how to install and configure it.
-  =REMOTE=  :: Use =http://flupy.org/data/flags=; that is a public website owned by me, hosted on a shared server. Please do not pound it with too many concurrent requests. The =flupy.org= domain is handled by a free account on the [[http://www.cloudflare.com/][Cloudflare CDN]] so you may notice that the first downloads are slower, but they get faster when the CDN cache warms up.^{[[[#ftn.id1079267][153]]]}
-  =DELAY=  :: Use =http://localhost:8002/flags=; a proxy delaying HTTP responses should be listening at port 8002. I used a Mozilla Vaurien in front of my local Nginx to introduce delays. The previously mentioned [[http://bit.ly/1JIsg2L][/README.rst/]] file has instructions for running a Vaurien proxy.
-  =ERROR=  :: Use =http://localhost:8003/flags=; a proxy introducing HTTP errors and delaying responses should be installed at port 8003. I used a different Vaurien configuration for this.

*** Warning
    :PROPERTIES:
    :CUSTOM_ID: warning-1
    :CLASS: title
    :END:

The =LOCAL= option only works if you configure and start a local HTTP server on port 8001. The =DELAY= and =ERROR= options require proxies listening on ports 8002 and 8003. Configuring Nginx and Mozilla Vaurien to enable these options is explained in the [[http://bit.ly/1JIsg2L][/17-futures/countries/README.rst/]] file in the [[https://github.com/fluentpython/example-code][/Fluent Python/ code repository]] on GitHub.

By default, each =flags2= script will fetch the flags of the 20 most populous countries from the =LOCAL= server (=http://localhost:8001/flags=) using a default number of concurrent connections, which varies from script to script. [[file:ch17.html#flags2_sequential_run][Example 17-9]] shows a sample run of the /flags2_sequential.py/ script using all defaults.



Example 17-9. Running flags2_sequential.py with all defaults: LOCAL site, top-20 flags, 1 concurrent connection

#+BEGIN_SRC screen
    $ python3 flags2_sequential.py
    LOCAL site: http://localhost:8001/flags
    Searching for 20 flags: from BD to VN
    1 concurrent connection will be used.
    --------------------
    20 flags downloaded.
    Elapsed time: 0.10s
#+END_SRC

You can select which flags will be downloaded in several ways. [[file:ch17.html#flags2_threadpool_run][Example 17-10]] shows how to download all flags with country codes starting with the letters A, B, or C.



Example 17-10. Run flags2_threadpool.py to fetch all flags with country codes prefixes A, B, or C from DELAY server

#+BEGIN_SRC screen
    $ python3 flags2_threadpool.py -s DELAY a b c
    DELAY site: http://localhost:8002/flags
    Searching for 78 flags: from AA to CZ
    30 concurrent connections will be used.
    --------------------
    43 flags downloaded.
    35 not found.
    Elapsed time: 1.72s
#+END_SRC

Regardless of how the country codes are selected, the number of flags to fetch can be limited with the =-l/--limit= option. [[file:ch17.html#flags2_asyncio_run][Example 17-11]] demonstrates how to run exactly 100 requests, combining the =-a= option to get all flags with =-l 100=.



Example 17-11. Run flags2_asyncio.py to get 100 flags (-al 100) from the ERROR server, using 100 concurrent requests (-m 100)

#+BEGIN_SRC screen
    $ python3 flags2_asyncio.py -s ERROR -al 100 -m 100
    ERROR site: http://localhost:8003/flags
    Searching for 100 flags: from AD to LK
    100 concurrent connections will be used.
    --------------------
    73 flags downloaded.
    27 errors.
    Elapsed time: 0.64s
#+END_SRC

That's the user interface of the =flags2= examples. Let's see how they are implemented.

*** Error Handling in the flags2 Examples
    :PROPERTIES:
    :CUSTOM_ID: _error_handling_in_the_flags2_examples
    :CLASS: title
    :END:

The common strategy adopted in all three examples to deal with HTTP errors is that 404 errors (Not Found) are handled by the function in charge of downloading a single file (=download_one=). Any other exception propagates to be handled by the =download_many= function.

Again, we'll start by studying the sequential code, which is easier to follow---and mostly reused by the thread pool script. [[file:ch17.html#flags2_basic_http_ex][Example 17-12]] shows the functions that perform the actual downloads in the /flags2_sequential.py/ and /flags2_threadpool.py/ scripts.



Example 17-12. flags2_sequential.py: basic functions in charge of downloading; both are reused in flags2_threadpool.py

#+BEGIN_EXAMPLE
    def get_flag(base_url, cc):
        url = '{}/{cc}/{cc}.gif'.format(base_url, cc=cc.lower())
        resp = requests.get(url)
        if resp.status_code != 200:   
            resp.raise_for_status()
        return resp.content


    def download_one(cc, base_url, verbose=False):
        try:
            image = get_flag(base_url, cc)
        except requests.exceptions.HTTPError as exc:   
            res = exc.response
            if res.status_code == 404:
                status = HTTPStatus.not_found   
                msg = 'not found'
            else:   
                raise
        else:
            save_flag(image, cc.lower() + '.gif')
            status = HTTPStatus.ok
            msg = 'OK'

        if verbose:   
            print(cc, msg)

        return Result(status, cc)   
#+END_EXAMPLE

- [[#CO202-1][[[file:callouts/1.png]]]]  :: =get_flag= does no error handling, it uses =requests.Response.raise_for_status= to raise an exception for any HTTP code other than 200.

- [[#CO202-2][[[file:callouts/2.png]]]]  :: =download_one= catches =requests.exceptions.HTTPError= to handle HTTP code 404 specifically...

- [[#CO202-3][[[file:callouts/3.png]]]]  :: ...by setting its local =status= to =HTTPStatus.not_found=; =HTTPStatus= is an =Enum= imported from =flags2_common= ([[file:apa.html#support_flag_utils][Example A-10]]).

- [[#CO202-4][[[file:callouts/4.png]]]]  :: Any other =HTTPError= exception is re-raised; other exceptions will just propagate to the caller.

- [[#CO202-5][[[file:callouts/5.png]]]]  :: If the =-v/--verbose= command-line option is set, the country code and status message will be displayed; this how you'll see progress in the verbose mode.

- [[#CO202-6][[[file:callouts/6.png]]]]  :: The =Result= =namedtuple= returned by =download_one= will have a =status= field with a value of =HTTPStatus.not_found= or =HTTPStatus.ok=.

[[file:ch17.html#flags2_dowload_many_seq][Example 17-13]] lists the sequential version of the =download_many= function. This code is straightforward, but its worth studying to contrast with the concurrent versions coming up. Focus on how it reports progress, handles errors, and tallies downloads.



Example 17-13. flags2_sequential.py: the sequential implementation of download_many

#+BEGIN_EXAMPLE
    def download_many(cc_list, base_url, verbose, max_req):
        counter = collections.Counter()   
        cc_iter = sorted(cc_list)   
        if not verbose:
            cc_iter = tqdm.tqdm(cc_iter)   
        for cc in cc_iter:   
            try:
                res = download_one(cc, base_url, verbose)   
            except requests.exceptions.HTTPError as exc:   
                error_msg = 'HTTP error {res.status_code} - {res.reason}'
                error_msg = error_msg.format(res=exc.response)
            except requests.exceptions.ConnectionError as exc:   
                error_msg = 'Connection error'
            else:   
                error_msg = ''
                status = res.status

            if error_msg:
                status = HTTPStatus.error   
            counter[status] += 1   
            if verbose and error_msg:  
                print('*** Error for {}: {}'.format(cc, error_msg))

        return counter   
#+END_EXAMPLE

- [[#CO203-1][[[file:callouts/1.png]]]]  :: This =Counter= will tally the different download outcomes: =HTTPStatus.ok=, =HTTPStatus.not_found=, or =HTTPStatus.error=.

- [[#CO203-2][[[file:callouts/2.png]]]]  :: =cc_iter= holds the list of the country codes received as arguments, ordered alphabetically.

- [[#CO203-3][[[file:callouts/3.png]]]]  :: If not running in verbose mode, =cc_iter= is passed to the =tqdm= function, which will return an iterator that yields the items in =cc_iter= while also displaying the animated progress bar.

- [[#CO203-4][[[file:callouts/4.png]]]]  :: This =for= loop iterates over =cc_iter= and...

- [[#CO203-5][[[file:callouts/5.png]]]]  :: ...performs the download by successive calls to =download_one=.

- [[#CO203-6][[[file:callouts/6.png]]]]  :: HTTP-related exceptions raised by =get_flag= and not handled by =download_one= are handled here.

- [[#CO203-7][[[file:callouts/7.png]]]]  :: Other network-related exceptions are handled here. Any other exception will abort the script, because the =flags2_common.main= function that calls =download_many= has no =try/except=.

- [[#CO203-8][[[file:callouts/8.png]]]]  :: If no exception escaped =download_one=, then the =status= is retrieved from the =HTTPStatus= =namedtuple= returned by =download_one=.

- [[#CO203-9][[[file:callouts/9.png]]]]  :: If there was an error, set the local =status= accordingly.

- [[#CO203-10][[[file:callouts/10.png]]]]  :: Increment the counter by using the value of the =HTTPStatus= =Enum= as key.

- [[#CO203-11][[[file:callouts/11.png]]]]  :: If running in verbose mode, display the error message for the current country code, if any.

- [[#CO203-12][[[file:callouts/12.png]]]]  :: Return the =counter= so that the =main= function can display the numbers in its final report.

We'll now study the refactored thread pool example, /flags2_threadpool.py/.

*** Using futures.as_completed
    :PROPERTIES:
    :CUSTOM_ID: using_futures_as_completed_sec
    :CLASS: title
    :END:

In order to integrate the TQDM progress bar and handle errors on each request, the /flags2_threadpool.py/ script uses =futures.ThreadPoolExecutor= with the =futures.as_completed= function we've already seen. [[file:ch17.html#flags2_threadpool_full][Example 17-14]] is the full listing of /flags2_threadpool.py/. Only the =download_many= function is implemented; the other functions are reused from the =flags2_common= and =flags2_sequential= modules.



Example 17-14. flags2_threadpool.py: full listing

#+BEGIN_EXAMPLE
    import collections
    from concurrent import futures

    import requests
    import tqdm   

    from flags2_common import main, HTTPStatus   
    from flags2_sequential import download_one   

    DEFAULT_CONCUR_REQ = 30   
    MAX_CONCUR_REQ = 1000   


    def download_many(cc_list, base_url, verbose, concur_req):
        counter = collections.Counter()
        with futures.ThreadPoolExecutor(max_workers=concur_req) as executor:   
            to_do_map = {}   
            for cc in sorted(cc_list):   
                future = executor.submit(download_one,
                                cc, base_url, verbose)   
                to_do_map[future] = cc   
            done_iter = futures.as_completed(to_do_map)   
            if not verbose:
                done_iter = tqdm.tqdm(done_iter, total=len(cc_list))   
            for future in done_iter:   
                try:
                    res = future.result()   
                except requests.exceptions.HTTPError as exc:   
                    error_msg = 'HTTP {res.status_code} - {res.reason}'
                    error_msg = error_msg.format(res=exc.response)
                except requests.exceptions.ConnectionError as exc:
                    error_msg = 'Connection error'
                else:
                    error_msg = ''
                    status = res.status

                if error_msg:
                    status = HTTPStatus.error
                counter[status] += 1
                if verbose and error_msg:
                    cc = to_do_map[future]   
                    print('*** Error for {}: {}'.format(cc, error_msg))

        return counter


    if __name__ == '__main__':
        main(download_many, DEFAULT_CONCUR_REQ, MAX_CONCUR_REQ)
#+END_EXAMPLE

- [[#CO204-1][[[file:callouts/1.png]]]]  :: Import the progress-bar display library.

- [[#CO204-2][[[file:callouts/2.png]]]]  :: Import one function and one =Enum= from the =flags2_common= module.

- [[#CO204-3][[[file:callouts/3.png]]]]  :: Reuse the =donwload_one= from =flags2_sequential= ([[file:ch17.html#flags2_basic_http_ex][Example 17-12]]).

- [[#CO204-4][[[file:callouts/4.png]]]]  :: If the =-m/--max_req= command-line option is not given, this will be the maximum number of concurrent requests, implemented as the size of the thread pool; the actual number may be smaller, if the number of flags to download is smaller.

- [[#CO204-5][[[file:callouts/5.png]]]]  :: =MAX_CONCUR_REQ= caps the maximum number of concurrent requests regardless of the number of flags to download or the =-m/--max_req= command-line option; it's a safety precaution.

- [[#CO204-6][[[file:callouts/6.png]]]]  :: Create the =executor= with =max_workers= set to =concur_req=, computed by the =main= function as the smaller of: =MAX_CONCUR_REQ=, the length of =cc_list=, and the value of the =-m/--max_req= command-line option. This avoids creating more threads than necessary.

- [[#CO204-7][[[file:callouts/7.png]]]]  :: This =dict= will map each =Future= instance---representing one download---with the respective country code for error reporting.

- [[#CO204-8][[[file:callouts/8.png]]]]  :: Iterate over the list of country codes in alphabetical order. The order of the results will depend on the timing of the HTTP responses more than anything, but if the size of the thread pool (given by =concur_req=) is much smaller than =len(cc_list)=, you may notice the downloads batched alphabetically.

- [[#CO204-9][[[file:callouts/9.png]]]]  :: Each call to =executor.submit= schedules the execution of one callable and returns a =Future= instance. The first argument is the callable, the rest are the arguments it will receive.

- [[#CO204-10][[[file:callouts/10.png]]]]  :: Store the =future= and the country code in the =dict=.

- [[#CO204-11][[[file:callouts/11.png]]]]  :: =futures.as_completed= returns an iterator that yields futures as they are done.

- [[#CO204-12][[[file:callouts/12.png]]]]  :: If not in verbose mode, wrap the result of =as_completed= with the =tqdm= function to display the progress bar; because =done_iter= has no =len=, we must tell =tqdm= what is the expected number of items as the =total== argument, so =tqdm= can estimate the work remaining.

- [[#CO204-13][[[file:callouts/13.png]]]]  :: Iterate over the futures as they are completed.

- [[#CO204-14][[[file:callouts/14.png]]]]  :: Calling the =result= method on a future either returns the value returned by the callable, or raises whatever exception was caught when the callable was executed. This method may block waiting for a resolution, but not in this example because =as_completed= only returns futures that are done.

- [[#CO204-15][[[file:callouts/15.png]]]]  :: Handle the potential exceptions; the rest of this function is identical to the sequential version of =download_many= ([[file:ch17.html#flags2_dowload_many_seq][Example 17-13]]), except for the next callout.

- [[#CO204-16][[[file:callouts/16.png]]]]  :: To provide context for the error message, retrieve the country code from the =to_do_map= using the current =future= as key. This was not necessary in the sequential version because we were iterating over the list of country codes, so we had the current =cc=; here we are iterating over the futures.

[[file:ch17.html#flags2_threadpool_full][Example 17-14]] uses an idiom that's very useful with =futures.as_completed=: building a =dict= to map each future to other data that may be useful when the future is completed. Here the =to_do_map= maps each future to the country code assigned to it. This makes it easy to do follow-up processing with the result of the futures, despite the fact that they are produced out of order.

Python threads are well suited for I/O-intensive applications, and the =concurrent.futures= package makes them trivially simple to use for certain use cases. This concludes our basic introduction to =concurrent.futures=. Let's now discuss alternatives for when =ThreadPoolExecutor= or =ProcessPoolExecutor= are not suitable.

*** Threading and Multiprocessing Alternatives
    :PROPERTIES:
    :CUSTOM_ID: _threading_and_multiprocessing_alternatives
    :CLASS: title
    :END:

Python has supported threads since its release 0.9.8 (1993); =concurrent.futures= is just the latest way of using them. In Python 3, the original =thread= module was deprecated in favor of the higher-level [[https://docs.python.org/3/library/threading.html][=threading= module]].^{[[[#ftn.id416812][154]]]} If =futures.ThreadPoolExecutor= is not flexible enough for a certain job, you may need to build your own solution out of basic =threading= components such as =Thread=, =Lock=, =Semaphore=, etc.---possibly using the thread-safe queues of the [[https://docs.python.org/3/library/queue.html][=queue= module]] for passing data between threads. Those moving parts are encapsulated by =futures.ThreadPoolExecutor=.

For CPU-bound work, you need to sidestep the GIL by launching multiple processes. The =futures.ProcessPoolExecutor= is the easiest way to do it. But again, if your use case is complex, you'll need more advanced tools. The [[https://docs.python.org/3/library/multiprocessing.html][=multiprocessing= package]] emulates the =threading= API but delegates jobs to multiple processes. For simple programs, =multiprocessing= can replace =threading= with few changes. But =multiprocessing= also offers facilities to solve the biggest challenge faced by collaborating processes: how to pass around data.

** Chapter Summary


We started the chapter by comparing two concurrent HTTP clients with a sequential one, demonstrating significant performance gains over the sequential script.

After studying the first example based on =concurrent.futures=, we took a closer look at future objects, either instances of =concurrent.futures.Future=, or =asyncio.Future=, emphasizing what these classes have in common (their differences will be emphasized in [[file:ch18.html][Chapter 18]]). We saw how to create futures by calling =Executor.submit(…)=, and iterate over completed futures with =concurrent.futures.as_completed(…)=.

Next, we saw why Python threads are well suited for I/O-bound applications, despite the GIL: every standard library I/O function written in C releases the GIL, so while a given thread is waiting for I/O, the Python scheduler can switch to another thread. We then discussed the use of multiple processes with the =concurrent.futures.ProcessPoolExecutor= class, to go around the GIL and use multiple CPU cores to run cryptographic algorithms, achieving speedups of more than 100% when using four workers.

In the following section, we took a close look at how the =concurrent.futures.ThreadPoolExecutor= works, with a didactic example launching tasks that did nothing for a few seconds, except displaying their status with a timestamp.

Next we went back to the flag downloading examples. Enhancing them with a progress bar and proper error handling prompted further exploration of the =future.as_completed= generator function showing a common pattern: storing futures in a =dict= to link further information to them when submitting, so that we can use that information when the future comes out of the =as_completed= iterator.

We concluded the coverage of concurrency with threads and processes with a brief reminder of the lower-level, but more flexible =threading= and =multiprocessing= modules, which represent the traditional way of leveraging threads and processes in Python.

** Further Reading


The =concurrent.futures= package was contributed by Brian Quinlan, who presented it in a great talk titled [[http://bit.ly/1JIuZJy][“The Future Is Soon!”]] at PyCon Australia 2010. Quinlan's talk has no slides; he shows what the library does by typing code directly in the Python console. As a motivating example, the presentation features a short video with XKCD cartoonist/programmer Randall Munroe making an unintended DOS attack on Google Maps to build a colored map of driving times around his city. The formal introduction to the library is [[https://www.python.org/dev/peps/pep-3148/][PEP 3148 - =futures= - execute computations asynchronously]]. In the PEP, Quinlan wrote that the =concurrent.futures= library was “heavily influenced by the Java =java.util.concurrent= package.”

/Parallel Programming with Python/ (Packt), by Jan Palach, covers several tools for concurrent programming, including the =concurrent.futures=, =threading=, and =multiprocessing= modules. It goes beyond the standard library to discuss [[http://bit.ly/1JIv1kA][Celery]], a task queue used to distribute work across threads and processes, even on different machines. In the Django community, Celery is probably the most widely used system to offload heavy tasks such as PDF generation to other processes, thus avoiding delays in producing an HTTP response.

In the Beazley and Jones /[[http://shop.oreilly.com/product/0636920027072.do][Python Cookbook, 3E]]/ (O'Reilly) there are recipes using =concurrent.futures= starting with “Recipe 11.12. Understanding Event-Driven I/O.” “Recipe 12.7. Creating a Thread Pool” shows a simple TCP echo server, and “Recipe 12.8. Performing Simple Parallel Programming” offers a very practical example: analyzing a whole directory of =gzip= compressed Apache logfiles with the help of a =ProcessPoolExecutor=. For more about threads, the entire Chapter 12 of Beazley and Jones is great, with special mention to “Recipe 12.10. Defining an Actor Task,” which demonstrates the Actor model: a proven way of coordinating threads through message passing.

Brett Slatkin's [[http://www.effectivepython.com/][/Effective Python/]] (Addison-Wesley) has a multitopic chapter about concurrency, including coverage of coroutines, =concurrent.futures= with threads and processes, and the use of locks and queues for thread programming without the =ThreadPoolExecutor=.

/[[http://shop.oreilly.com/product/0636920028963.do][High Performance Python]]/ (O'Reilly) by Micha Gorelick and Ian Ozsvald and /The Python Standard Library by Example/ (Addison-Wesley), by Doug Hellmann, also cover threads and processes.

For a modern take on concurrency without threads or callbacks, /Seven Concurrency Models in Seven Weeks/, by Paul Butcher (Pragmatic Bookshelf) is an excellent read. I love its subtitle: “When Threads Unravel.” In that book, threads and locks are covered in Chapter 1, and the remaining six chapters are devoted to modern alternatives to concurrent programming, as supported by different languages. Python, Ruby, and JavaScript are not among them.

If you are intrigued about the GIL, start with the /Python Library and Extension FAQ/ ([[http://bit.ly/1HGtb0F][“Can't we get rid of the Global Interpreter Lock?”]]). Also worth reading are posts by Guido van Rossum and Jesse Noller (contributor of the =multiprocessing= package): [[http://bit.ly/1HGtcBF][“It isn't Easy to Remove the GIL”]] and [[http://bit.ly/1JIvgwd][“Python Threads and the Global Interpreter Lock.”]] Finally, David Beazley has a detailed exploration on the inner workings of the GIL: [[http://www.dabeaz.com/GIL/][“Understanding the Python GIL.”]]^{[[[#ftn.id963851][155]]]} In slide #54 of the [[http://bit.ly/1HGtCrK][presentation]], Beazley reports some alarming results, including a 20× increase in processing time for a particular benchmark with the new GIL algorithm introduced in Python 3.2. However, Beazley apparently used an empty =while True: pass= to simulate CPU-bound work, and that is not realistic. The issue is not significant with real workloads, according to [[http://bugs.python.org/issue7946#msg223110][a comment]] by Antoine Pitrou---who implemented the new GIL algorithm---in the bug report submitted by Beazley.

While the GIL is real problem and is not likely to go away soon, Jesse Noller and Richard Oudkerk contributed a library to make it easier to work around it in CPU-bound applications: the =multiprocessing= package, which emulates the =threading= API across processes, along with supporting infrastructure of locks, queues, pipes, shared memory, etc. The package was introduced in [[https://www.python.org/dev/peps/pep-0371/][PEP 371 --- Addition of the multiprocessing package to the standard library]]. The [[http://bit.ly/multi-docs][official documentation for the package]] is a 93 KB /.rst/ file---that's about 63 pages---making it one of the longest chapters in the Python standard library. Multiprocessing is the basis for the =concurrent.futures.ProcessPoolExecutor=.

For CPU- and data-intensive parallel processing, a new option with a lot of momentum in the big data community is the [[https://spark.apache.org][Apache Spark]] distributed computing engine, offering a friendly Python API and support for Python objects as data, as shown in their [[https://spark.apache.org/examples.html][examples page]].

Two elegant and super easy libraries for parallelizing tasks over processes are [[https://pypi.python.org/pypi/lelo][=lelo=]] by João S. O. Bueno and [[http://bit.ly/1HGtF6Q][=python-parallelize=]] by Nat Pryce. The =lelo= package defines a =@parallel= decorator that you can apply to any function to magically make it unblocking: when you call the decorated function, its execution is started in another process. Nat Pryce's =python-parallelize= package provides a =parallelize= generator that you can use to distribute the execution of a =for= loop over multiple CPUs. Both packages use the =multiprocessing= module under the covers.

Soapbox

*Thread Avoidance*

#+BEGIN_QUOTE
  Concurrency: one of the most difficult topics in computer science (usually best avoided).^{[[[#ftn.id748281][156]]]}

  --- David Beazley /Python coach and mad scientist/

#+END_QUOTE

I agree with the apparently contradictory quotes by David Beazley, above, and Michele Simionato at the start of this chapter. After attending a concurrency course at the university---in which “concurrent programming” was equated to managing threads and locks---I came to the conclusion that I don't want to manage threads and locks myself, any more than I want to manage memory allocation and deallocation. Those jobs are best carried out by the systems programmers who have the know-how, the inclination, and the time to get them right---hopefully.

That's why I think the =concurrent.futures= package is exciting: it treats threads, processes, and queues as infrastructure at your service, not something you have to deal with directly. Of course, it's designed with simple jobs in mind, the so-called [[http://bit.ly/1HGtGaR][“embarrassingly parallel”]] problems. But that's a large slice of the concurrency problems we face when writing applications---as opposed to operating systems or database servers, as Simionato points out in that quote.

For “nonembarrassing” concurrency problems, threads and locks are not the answer either. Threads will never disappear at the OS level, but every programming language I've found exciting in the last several years provides better, higher-level, concurrency abstractions, as the /Seven Concurrency Models/ book demonstrates. Go, Elixir, and Clojure are among them. Erlang---the implementation language of Elixir---is a prime example of a language designed from the ground up with concurrency in mind. It doesn't excite me for a simple reason: I find its syntax ugly. Python spoiled me that way.

José Valim, well-known as a Ruby on Rails core contributor, designed Elixir with a pleasant, modern syntax. Like Lisp and Clojure, Elixir implements syntactic macros. That's a double-edged sword. Syntactic macros enable powerful DSLs, but the proliferation of sublanguages can lead to incompatible codebases and community fragmentation. Lisp drowned in a flood of macros, with each Lisp shop using its own arcane dialect. Standardizing around Common Lisp resulted in a bloated language. I hope José Valim can inspire the Elixir community to avoid a similar outcome.

Like Elixir, Go is a modern language with fresh ideas. But, in some regards, it's a conservative language, compared to Elixir. Go doesn't have macros, and its syntax is simpler than Python's. Go doesn't support inheritance or operator overloading, and it offers fewer opportunities for metaprogramming than Python. These limitations are considered features. They lead to more predictable behavior and performance. That's a big plus in the highly concurrent, mission-critical settings where Go aims to replace C++, Java, and Python.

While Elixir and Go are direct competitors in the high-concurrency space, their design philosophies appeal to different crowds. Both are likely to thrive. But in the history of programming languages, the conservative ones tend to attract more coders. I'd like to become fluent in Go and Elixir.

*About the GIL*

The GIL simplifies the implementation of the CPython interpreter and of extensions written in C, so we can thank the GIL for the vast number of extensions in C available for Python---and that is certainly one of the key reasons why Python is so popular today.

For many years, I was under the impression that the GIL made Python threads nearly useless beyond toy applications. It was not until I discovered that /every/ blocking I/O call in the standard library releases the GIL that I realized Python threads are excellent for I/O-bound systems---the kind of applications customers usually pay me to develop, given my professional experience.

*Concurrency in the Competition*

MRI---the reference implementation of Ruby---also has a GIL, so its threads are under the same limitations as Python's. Meanwhile, JavaScript interpreters don't support user-level threads at all; asynchronous programming with callbacks is their only path to concurrency. I mention this because Ruby and JavaScript are the closest direct competitors to Python as general-purpose, dynamic programming languages.

Looking at the concurrency-savvy new crop of languages, Go and Elixir are probably the ones best positioned to eat Python's lunch. But now we have =asyncio=. If hordes of people believe Node.js with raw callbacks is a viable platform for concurrent programming, how hard can it be to win them over to Python when the =asyncio= ecosystem matures? But that's a topic for the next [[file:ch18.html#ch18-soapbox][Soapbox]].



--------------


^{[[[#id789805][148]]]} From Michele Simionato's post [[http://bit.ly/1JIrYZQ][Threads, processes and concurrency in Python: some thoughts]], subtitled “Removing the hype around the multicore (non) revolution and some (hopefully) sensible comment about threads and other forms of concurrency.”


^{[[[#id507235][149]]]} The images are originally from the [[http://1.usa.gov/1JIsmHJ][CIA World Factbook]], a public-domain, U.S. government publication. I copied them to my site to avoid the risk of launching a DOS attack on CIA.gov.


^{[[[#id440956][150]]]} This is a limitation of the CPython interpreter, not of the Python language itself. Jython and IronPython are not limited in this way; but Pypy, the fastest Python interpreter available, also has a GIL.


^{[[[#id441026][151]]]} Slide 106 of [[http://www.dabeaz.com/finalgenerator/][“Generators: The Final Frontier”]].


^{[[[#id1079796][152]]]} Your mileage may vary: with threads, you never know the exact sequencing of events that should happen practically at the same time; it's possible that, in another machine, you see =loiter(1)= starting before =loiter(0)= finishes, particularly because =sleep= always releases the GIL so Python may switch to another thread even if you sleep for 0s.


^{[[[#id1079267][153]]]} Before configuring Cloudflare, I got HTTP 503 errors---Service Temporarily Unavailable---when testing the scripts with a few dozen concurrent requests on my inexpensive shared host account. Now those errors are gone.


^{[[[#id416812][154]]]} The =threading= module has been available since Python 1.5.1 (1998), yet some insist on using the old =thread= module. In Python 3, it was renamed to =_thread= to highlight the fact that it's just a low-level implementation detail, and shouldn't be used in application code.


^{[[[#id963851][155]]]} Thanks to Lucas Brunialti for sending me a link to this talk.


^{[[[#id748281][156]]]} Slide #9 from [[http://www.dabeaz.com/coroutines/][“A Curious Course on Coroutines and Concurrency,”]] tutorial presented at PyCon 2009.


or/][“Generators: The Final Frontier”]].


^{[[[#id1079796][152]]]} Your mileage may vary: with threads, you never know the exact sequencing of events that should happen practically at the same time; it's possible that, in another machine, you see =loiter(1)= starting before =loiter(0)= finishes, particularly because =sleep= always releases the GIL so Python may switch to another thread even if you sleep for 0s.


^{[[[#id1079267][153]]]} Before configuring Cloudflare, I got HTTP 503 errors---Service Temporarily Unavailable---when testing the scripts with a few dozen concurrent requests on my inexpensive shared host account. Now those errors are gone.


^{[[[#id416812][154]]]} The =threading= module has been available since Python 1.5.1 (1998), yet some insist on using the old =thread= module. In Python 3, it was renamed to =_thread= to highlight the fact that it's just a low-level implementation detail, and shouldn't be used in application code.


^{[[[#id963851][155]]]} Thanks to Lucas Brunialti for sending me a link to this talk.


^{[[[#id748281][156]]]} Slide #9 from [[http://www.dabeaz.com/coroutines/][“A Curious Course on Coroutines and Concurrency,”]] tutorial presented at PyCon 2009.


us Course on Coroutines and Concurrency,”]] tutorial presented at PyCon 2009.


